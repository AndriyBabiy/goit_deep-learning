{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86259,"databundleVersionId":9778966,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Library Imports and Environment Setup","metadata":{}},{"cell_type":"code","source":"# Install required packages (if not already installed)\n!pip install transformers torch torchvision\n\n# Standard library imports\nimport os\nimport gc\nimport warnings\nfrom collections import defaultdict, Counter\n\n# Set environment variables\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nwarnings.filterwarnings(\"ignore\")\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    confusion_matrix\n)\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom transformers import RobertaTokenizer, RobertaModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:31.367740Z","iopub.execute_input":"2024-10-21T22:06:31.368111Z","iopub.status.idle":"2024-10-21T22:06:51.006582Z","shell.execute_reply.started":"2024-10-21T22:06:31.368072Z","shell.execute_reply":"2024-10-21T22:06:51.005699Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Global configuration elements","metadata":{}},{"cell_type":"code","source":"class CFG:\n    BASE_DIR = \"/kaggle/input/deep-learning-for-computer-vision-and-nlp-2024-10/\"\n    TRAIN_CSV = os.path.join(BASE_DIR, \"train.csv\")\n    TEST_CSV = os.path.join(BASE_DIR, \"test.csv\")\n    TRAIN_IMAGES_DIR = os.path.join(BASE_DIR, \"images/images/train\")\n    TEST_IMAGES_DIR = os.path.join(BASE_DIR, \"images/images/test\")\n    OUTPUT_DIR = '/kaggle/working/'\n    IMAGE_SIZE = 224\n    MAX_LENGTH = 256\n    BATCH_SIZE = 16  # Adjust based on your system's capacity\n    NUM_EPOCHS = 5\n    NUM_CLASSES = 4  # Assuming classes are 1 to 4\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    LEARNING_RATE = 1e-4\n    RANDOM_STATE = 42\n\n# Set random seed for reproducibility\ndef set_seed(seed=CFG.RANDOM_STATE):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:51.008312Z","iopub.execute_input":"2024-10-21T22:06:51.008812Z","iopub.status.idle":"2024-10-21T22:06:51.052184Z","shell.execute_reply.started":"2024-10-21T22:06:51.008777Z","shell.execute_reply":"2024-10-21T22:06:51.051407Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data Loading and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Load training and test data","metadata":{}},{"cell_type":"code","source":"# Load training and test data\ntrain_df = pd.read_csv(CFG.TRAIN_CSV, dtype=str, keep_default_na=False)\ntest_df = pd.read_csv(CFG.TEST_CSV, dtype=str, keep_default_na=False)\n\n# Convert data types\ntrain_df['AdoptionSpeed'] = pd.to_numeric(train_df['AdoptionSpeed'])\ntrain_df['PetID'] = train_df['PetID'].astype(str)\ntest_df['PetID'] = test_df['PetID'].astype(str)\n\n# Display basic information\nprint(\"Training Data Shape:\", train_df.shape)\nprint(\"Test Data Shape:\", test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:51.053329Z","iopub.execute_input":"2024-10-21T22:06:51.053800Z","iopub.status.idle":"2024-10-21T22:06:51.186175Z","shell.execute_reply.started":"2024-10-21T22:06:51.053766Z","shell.execute_reply":"2024-10-21T22:06:51.185246Z"}},"outputs":[{"name":"stdout","text":"Training Data Shape: (6431, 3)\nTest Data Shape: (1891, 2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Preparing image paths","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\n# Function to create a mapping from PetID to image file paths\ndef get_id_to_images(images_folder):\n    image_filenames = os.listdir(images_folder)\n    id_to_images = defaultdict(list)\n    for filename in image_filenames:\n        if filename.endswith(('.jpg', '.png', '.jpeg')):\n            id_part = filename.split('-')[0]\n            id_to_images[id_part].append(os.path.join(images_folder, filename))\n    return id_to_images\n\n# Get mappings for training and test images\ntrain_id_to_images = get_id_to_images(CFG.TRAIN_IMAGES_DIR)\ntest_id_to_images = get_id_to_images(CFG.TEST_IMAGES_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:51.188861Z","iopub.execute_input":"2024-10-21T22:06:51.189266Z","iopub.status.idle":"2024-10-21T22:06:51.710429Z","shell.execute_reply.started":"2024-10-21T22:06:51.189220Z","shell.execute_reply":"2024-10-21T22:06:51.709500Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Prepare Text Data and Labels","metadata":{}},{"cell_type":"code","source":"# Prepare text data\ntrain_text_data = dict(zip(train_df['PetID'], train_df['Description']))\ntest_text_data = dict(zip(test_df['PetID'], test_df['Description']))\n\n# Prepare labels (ensure labels are integers)\ntrain_labels = dict(zip(train_df['PetID'], train_df['AdoptionSpeed'].astype(int)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:51.711871Z","iopub.execute_input":"2024-10-21T22:06:51.712252Z","iopub.status.idle":"2024-10-21T22:06:51.723718Z","shell.execute_reply.started":"2024-10-21T22:06:51.712209Z","shell.execute_reply":"2024-10-21T22:06:51.722748Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Dataset and DataLoader setup","metadata":{}},{"cell_type":"markdown","source":"## Define the Multimodal Dataset Class","metadata":{}},{"cell_type":"code","source":"class MultimodalDataset(Dataset):\n    def __init__(self, text_data, labels=None, id_to_images=None, tokenizer=None, transform=None, max_length=256):\n        self.ids = list(text_data.keys())\n        self.text_data = text_data\n        self.labels = labels  # Labels can be None\n        self.id_to_images = id_to_images if id_to_images is not None else {}\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        id_ = self.ids[idx]\n        text = self.text_data[id_]\n\n        # Tokenize text\n        encoded = self.tokenizer.encode_plus(\n            text if text else \"\",  # Handle missing descriptions\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        input_ids = encoded['input_ids'].squeeze(0)\n        attention_mask = encoded['attention_mask'].squeeze(0)\n\n        # Load images associated with the ID\n        image_paths = self.id_to_images.get(id_, [])\n        images = []\n        for image_path in image_paths:\n            image = Image.open(image_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            images.append(image)\n        if images:\n            images = torch.stack(images)\n        else:\n            # Handle case where no images are found\n            image_size = self.transform.transforms[0].size[0] if self.transform else CFG.IMAGE_SIZE\n            images = torch.zeros((1, 3, image_size, image_size))\n\n        # Prepare the sample dictionary\n        sample = {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'images': images,\n            'id': id_\n        }\n\n        # Include labels only if they are available\n        if self.labels is not None:\n            label = self.labels[id_]\n            label = label - 1 \n            label = torch.tensor(label, dtype=torch.long)\n            sample['label'] = label\n\n        return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:51.724887Z","iopub.execute_input":"2024-10-21T22:06:51.725180Z","iopub.status.idle":"2024-10-21T22:06:51.737928Z","shell.execute_reply.started":"2024-10-21T22:06:51.725150Z","shell.execute_reply":"2024-10-21T22:06:51.737102Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Define the Collate Function","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = torch.stack([item['input_ids'] for item in batch])\n    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n\n    # Handle images\n    images = [item['images'] for item in batch]\n    max_num_images = max([img.shape[0] for img in images])\n    padded_images = []\n    for img in images:\n        num_images = img.shape[0]\n        if num_images < max_num_images:\n            # Pad with zeros\n            padding = torch.zeros((max_num_images - num_images, *img.shape[1:]))\n            img = torch.cat([img, padding], dim=0)\n        padded_images.append(img)\n    images = torch.stack(padded_images)\n\n    ids = [item['id'] for item in batch]\n\n    batch_dict = {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'images': images,\n        'id': ids\n    }\n\n    # Include labels if they are present\n    if 'label' in batch[0]:\n        labels = torch.stack([item['label'] for item in batch])\n        batch_dict['label'] = labels\n\n    return batch_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:51.738983Z","iopub.execute_input":"2024-10-21T22:06:51.739257Z","iopub.status.idle":"2024-10-21T22:06:51.755124Z","shell.execute_reply.started":"2024-10-21T22:06:51.739227Z","shell.execute_reply":"2024-10-21T22:06:51.754311Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Initialize Tokenizer and Transforms","metadata":{}},{"cell_type":"code","source":"# Initialize tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Define image transformations\nimage_transforms = transforms.Compose([\n    transforms.Resize((CFG.IMAGE_SIZE, CFG.IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:51.756008Z","iopub.execute_input":"2024-10-21T22:06:51.756278Z","iopub.status.idle":"2024-10-21T22:06:53.343711Z","shell.execute_reply.started":"2024-10-21T22:06:51.756248Z","shell.execute_reply":"2024-10-21T22:06:53.342801Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b0c704798f4af0afb935a2113076fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bfd2f2ef0fa4e36b0a70c34f8f65104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc223c18fbee4c65b9878affaeb869cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83522e2c5e14f8f9a757563c56fee6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"670448a1ba7a48dbb1e83de02c200e92"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Model Definition","metadata":{}},{"cell_type":"markdown","source":"## Textmodel","metadata":{}},{"cell_type":"code","source":"class TextModel(nn.Module):\n    def __init__(self):\n        super(TextModel, self).__init__()\n        self.roberta = RobertaModel.from_pretrained('distilroberta-base')\n        self.dropout = nn.Dropout(p=0.3)\n        self.output_dim = 256\n        self.fc = nn.Linear(self.roberta.config.hidden_size, self.output_dim)\n        \n        # Freeze some layers if necessary\n        for param in self.roberta.embeddings.parameters():\n            param.requires_grad = False\n        for param in self.roberta.encoder.layer[:2].parameters():  # Freeze first 2 layers\n            param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        x = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:53.345027Z","iopub.execute_input":"2024-10-21T22:06:53.345765Z","iopub.status.idle":"2024-10-21T22:06:53.353614Z","shell.execute_reply.started":"2024-10-21T22:06:53.345719Z","shell.execute_reply":"2024-10-21T22:06:53.352729Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## ImageModel","metadata":{}},{"cell_type":"code","source":"class ImageModel(nn.Module):\n    def __init__(self):\n        super(ImageModel, self).__init__()\n        self.model = models.resnet18(pretrained=True)\n        self.model.fc = nn.Identity()\n        self.output_dim = 512  # ResNet18's output dimension\n        \n        # Freeze some layers if necessary\n        for param in self.model.parameters():\n            param.requires_grad = False\n        for param in self.model.layer4.parameters():  # Unfreeze last block\n            param.requires_grad = True\n\n    def forward(self, images):\n        # images shape: [batch_size, max_num_images, channels, height, width]\n        batch_size, num_images, channels, height, width = images.shape\n        images = images.view(-1, channels, height, width)  # Flatten batch and images\n        features = self.model(images)  # Shape: [batch_size * num_images, feature_dim]\n        features = features.view(batch_size, num_images, -1)\n        features = torch.mean(features, dim=1)  # Aggregate features by averaging\n        return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:53.357053Z","iopub.execute_input":"2024-10-21T22:06:53.357346Z","iopub.status.idle":"2024-10-21T22:06:53.370096Z","shell.execute_reply.started":"2024-10-21T22:06:53.357316Z","shell.execute_reply":"2024-10-21T22:06:53.369276Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## MultiModal Model","metadata":{}},{"cell_type":"code","source":"class MultimodalModel(nn.Module):\n    def __init__(self, text_model, image_model, num_classes):\n        super(MultimodalModel, self).__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        combined_feature_dim = self.text_model.output_dim + self.image_model.output_dim\n        self.classifier = nn.Sequential(\n            nn.Linear(combined_feature_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        text_features = self.text_model(input_ids, attention_mask)\n        image_features = self.image_model(images)\n        combined = torch.cat((text_features, image_features), dim=1)\n        output = self.classifier(combined)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:53.371287Z","iopub.execute_input":"2024-10-21T22:06:53.371619Z","iopub.status.idle":"2024-10-21T22:06:53.382455Z","shell.execute_reply.started":"2024-10-21T22:06:53.371587Z","shell.execute_reply":"2024-10-21T22:06:53.381532Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"markdown","source":"## Split data into training and validation sets","metadata":{}},{"cell_type":"code","source":"# Get list of IDs\nids = list(train_text_data.keys())\n\n# Split IDs into training and validation sets\ntrain_ids, val_ids = train_test_split(ids, test_size=0.2, random_state=CFG.RANDOM_STATE)\n\n# Create training and validation datasets\ntrain_dataset = MultimodalDataset(\n    text_data={id_: train_text_data[id_] for id_ in train_ids},\n    labels={id_: train_labels[id_] for id_ in train_ids},\n    id_to_images=train_id_to_images,\n    tokenizer=tokenizer,\n    transform=image_transforms,\n    max_length=CFG.MAX_LENGTH\n)\n\nval_dataset = MultimodalDataset(\n    text_data={id_: train_text_data[id_] for id_ in val_ids},\n    labels={id_: train_labels[id_] for id_ in val_ids},\n    id_to_images=train_id_to_images,\n    tokenizer=tokenizer,\n    transform=image_transforms,\n    max_length=CFG.MAX_LENGTH\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:53.383586Z","iopub.execute_input":"2024-10-21T22:06:53.383863Z","iopub.status.idle":"2024-10-21T22:06:53.404153Z","shell.execute_reply.started":"2024-10-21T22:06:53.383834Z","shell.execute_reply":"2024-10-21T22:06:53.403383Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Create DataLoaders","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_dataset,\n    batch_size=CFG.BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4,\n    pin_memory=True\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=CFG.BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=4,\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:53.405233Z","iopub.execute_input":"2024-10-21T22:06:53.405645Z","iopub.status.idle":"2024-10-21T22:06:53.415769Z","shell.execute_reply.started":"2024-10-21T22:06:53.405613Z","shell.execute_reply":"2024-10-21T22:06:53.414643Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Initializing models and optimizer","metadata":{}},{"cell_type":"code","source":"# Initialize models\ntext_model = TextModel().to(CFG.DEVICE)\nimage_model = ImageModel().to(CFG.DEVICE)\nmodel = MultimodalModel(text_model, image_model, num_classes=CFG.NUM_CLASSES).to(CFG.DEVICE)\n\n# Define optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=CFG.LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:53.416982Z","iopub.execute_input":"2024-10-21T22:06:53.417348Z","iopub.status.idle":"2024-10-21T22:06:56.153870Z","shell.execute_reply.started":"2024-10-21T22:06:53.417307Z","shell.execute_reply":"2024-10-21T22:06:56.152693Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf4b00a49f934549bee6c2776df903bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a75b39d06a4c95b18938d80476b4cd"}},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 184MB/s]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Training Loop","metadata":{}},{"cell_type":"code","source":"# # In your training loop, after loading labels\n# labels = batch['label'].to(CFG.DEVICE)\n# labels = labels.long()\n\n# # Debugging: Print label information\n# if batch_idx == 0 and epoch == 0:\n#     print(\"Labels min:\", labels.min().item())\n#     print(\"Labels max:\", labels.max().item())\n#     print(\"Labels unique values:\", labels.unique())\n#     print(\"Labels shape:\", labels.shape)\n#     print(\"Labels dtype:\", labels.dtype)\n#     print(\"Labels device:\", labels.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:06:56.155544Z","iopub.execute_input":"2024-10-21T22:06:56.155853Z","iopub.status.idle":"2024-10-21T22:06:56.551861Z","shell.execute_reply.started":"2024-10-21T22:06:56.155821Z","shell.execute_reply":"2024-10-21T22:06:56.545291Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# In your training loop, after loading labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Debugging: Print label information\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"],"ename":"NameError","evalue":"name 'batch' is not defined","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\nbest_val_accuracy = 0.0\n\nfor epoch in range(CFG.NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(CFG.DEVICE)\n        attention_mask = batch['attention_mask'].to(CFG.DEVICE)\n        images = batch['images'].to(CFG.DEVICE)\n        labels = batch['label'].to(CFG.DEVICE)\n        labels = labels.long()\n        \n        # Debugging: Verify labels\n        if batch_idx == 0 and epoch == 0:\n            print(\"Labels min:\", labels.min().item())  # Should be 0\n            print(\"Labels max:\", labels.max().item())  # Should be 3\n            print(\"Labels unique values:\", labels.unique())\n            print(\"Labels dtype:\", labels.dtype)  # Should be torch.int64 or torch.long\n\n        with autocast():\n            outputs = model(input_ids, attention_mask, images)\n            loss = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{CFG.NUM_EPOCHS}, Training Loss: {avg_train_loss:.4f}\")\n\n    # Evaluate on validation set\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in val_dataloader:\n            input_ids = batch['input_ids'].to(CFG.DEVICE)\n            attention_mask = batch['attention_mask'].to(CFG.DEVICE)\n            images = batch['images'].to(CFG.DEVICE)\n            labels = batch['label'].to(CFG.DEVICE)\n            labels = labels.long()\n\n            outputs = model(input_ids, attention_mask, images)\n            _, preds = torch.max(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate validation metrics\n    val_accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n\n    # Adjust learning rate\n    scheduler.step(avg_train_loss)\n\n    # Save the best model\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(\"Best model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:07:25.154080Z","iopub.execute_input":"2024-10-21T22:07:25.155264Z","iopub.status.idle":"2024-10-21T22:18:58.939972Z","shell.execute_reply.started":"2024-10-21T22:07:25.155218Z","shell.execute_reply":"2024-10-21T22:18:58.938672Z"}},"outputs":[{"name":"stdout","text":"Labels min: 0\nLabels max: 3\nLabels unique values: tensor([0, 1, 2, 3], device='cuda:0')\nLabels dtype: torch.int64\nEpoch 1/5, Training Loss: 1.2933\nValidation Accuracy: 0.4204\nBest model saved.\nEpoch 2/5, Training Loss: 1.2200\nValidation Accuracy: 0.4693\nBest model saved.\nEpoch 3/5, Training Loss: 1.1538\nValidation Accuracy: 0.4662\nEpoch 4/5, Training Loss: 1.0595\nValidation Accuracy: 0.4740\nBest model saved.\nEpoch 5/5, Training Loss: 0.8735\nValidation Accuracy: 0.4079\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Evaluating Model","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(CFG.DEVICE)\n            attention_mask = batch['attention_mask'].to(CFG.DEVICE)\n            images = batch['images'].to(CFG.DEVICE)\n            labels = batch['label'].to(CFG.DEVICE)\n            labels = labels.long()\n\n            outputs = model(input_ids, attention_mask, images)\n            _, preds = torch.max(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='weighted'\n    )\n    cm = confusion_matrix(all_labels, all_preds)\n\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n\n    return accuracy, precision, recall, f1, cm\n\n# Evaluate on validation set\nevaluate(model, val_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:18:58.942794Z","iopub.execute_input":"2024-10-21T22:18:58.943631Z","iopub.status.idle":"2024-10-21T22:19:24.548665Z","shell.execute_reply.started":"2024-10-21T22:18:58.943586Z","shell.execute_reply":"2024-10-21T22:19:24.547594Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.4079\nPrecision: 0.4372, Recall: 0.4079, F1-Score: 0.4037\nConfusion Matrix:\n[[116  94  13  19]\n [ 86 179  29  48]\n [ 56 113  51  51]\n [ 77 131  45 179]]\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(0.40792540792540793,\n 0.43723611750515334,\n 0.40792540792540793,\n 0.4037054943548988,\n array([[116,  94,  13,  19],\n        [ 86, 179,  29,  48],\n        [ 56, 113,  51,  51],\n        [ 77, 131,  45, 179]]))"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"# Making Predictions on the Test Data","metadata":{}},{"cell_type":"markdown","source":"## Preparing the test dataset","metadata":{}},{"cell_type":"code","source":"# Create test dataset\ntest_dataset = MultimodalDataset(\n    text_data=test_text_data,\n    labels=None,  # No labels provided\n    id_to_images=test_id_to_images,\n    tokenizer=tokenizer,\n    transform=image_transforms,\n    max_length=CFG.MAX_LENGTH\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=CFG.BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=4,\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:19:24.550336Z","iopub.execute_input":"2024-10-21T22:19:24.550670Z","iopub.status.idle":"2024-10-21T22:19:24.556224Z","shell.execute_reply.started":"2024-10-21T22:19:24.550634Z","shell.execute_reply":"2024-10-21T22:19:24.555328Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Loading the best model","metadata":{}},{"cell_type":"code","source":"# Load the saved best model\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.to(CFG.DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:19:24.558724Z","iopub.execute_input":"2024-10-21T22:19:24.559091Z","iopub.status.idle":"2024-10-21T22:19:24.901603Z","shell.execute_reply.started":"2024-10-21T22:19:24.559050Z","shell.execute_reply":"2024-10-21T22:19:24.900680Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"MultimodalModel(\n  (text_model): TextModel(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): RobertaEncoder(\n        (layer): ModuleList(\n          (0-5): 6 x RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSdpaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): RobertaPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.3, inplace=False)\n    (fc): Linear(in_features=768, out_features=256, bias=True)\n  )\n  (image_model): ImageModel(\n    (model): ResNet(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n      (fc): Identity()\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=768, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=128, out_features=4, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Defining the prediction function","metadata":{}},{"cell_type":"code","source":"def predict(model, dataloader):\n    model.eval()\n    predictions = {}\n    with torch.no_grad():\n        for batch in dataloader:\n            ids = batch['id']\n            input_ids = batch['input_ids'].to(CFG.DEVICE)\n            attention_mask = batch['attention_mask'].to(CFG.DEVICE)\n            images = batch['images'].to(CFG.DEVICE)\n\n            outputs = model(input_ids, attention_mask, images)\n            _, preds = torch.max(outputs, dim=1)\n            preds = preds.cpu().numpy() + 1  # Adjust back to [1, 4]\n\n            for id_, pred in zip(ids, preds):\n                predictions[id_] = pred\n\n    return predictions\n\n# Run predictions on test data\ntest_predictions = predict(model, test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:19:24.903019Z","iopub.execute_input":"2024-10-21T22:19:24.903344Z","iopub.status.idle":"2024-10-21T22:20:11.716769Z","shell.execute_reply.started":"2024-10-21T22:19:24.903311Z","shell.execute_reply":"2024-10-21T22:20:11.715672Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generating the submission file","metadata":{}},{"cell_type":"code","source":"# Convert predictions to DataFrame\nsubmission_df = pd.DataFrame(list(test_predictions.items()), columns=['PetID', 'AdoptionSpeed'])\n\n# Ensure AdoptionSpeed is integer\nsubmission_df['AdoptionSpeed'] = submission_df['AdoptionSpeed'].astype(int)\n\n# Save submission file\nsubmission_df.to_csv(os.path.join(CFG.OUTPUT_DIR, 'submission.csv'), index=False)\n\nprint(\"Submission file saved at:\", os.path.join(CFG.OUTPUT_DIR, 'submission.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:25:29.313756Z","iopub.execute_input":"2024-10-21T22:25:29.314623Z","iopub.status.idle":"2024-10-21T22:25:29.334277Z","shell.execute_reply.started":"2024-10-21T22:25:29.314581Z","shell.execute_reply":"2024-10-21T22:25:29.333313Z"}},"outputs":[{"name":"stdout","text":"Submission file saved at: /kaggle/working/submission.csv\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"final_submission = submission_df\n\nprint(final_submission)\nfinal_submission.hist('AdoptionSpeed')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:31:00.251648Z","iopub.execute_input":"2024-10-21T22:31:00.252669Z","iopub.status.idle":"2024-10-21T22:31:00.609113Z","shell.execute_reply.started":"2024-10-21T22:31:00.252606Z","shell.execute_reply":"2024-10-21T22:31:00.608141Z"}},"outputs":[{"name":"stdout","text":"          PetID  AdoptionSpeed\n0     6697a7f62              4\n1     23b64fe21              2\n2     41e824cbe              4\n3     6c3d7237b              4\n4     97b0b5d92              4\n...         ...            ...\n1886  986e26eeb              2\n1887  9b2316d19              4\n1888  c60193e34              1\n1889  4f7a70728              3\n1890  9e758c0b0              1\n\n[1891 rows x 2 columns]\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"array([[<Axes: title={'center': 'AdoptionSpeed'}>]], dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4wUlEQVR4nO3de3xU1b3///ckmUwIMEEoSYiEiDcgIEK5ZdTHF4WQiCnesBVFGpWqhwYV0uOFcywEsEWpFYsGwRbBWnhQaRUVEQiocCrhFsUCWioWgQpJtEgCRCZDsn5/+Mu0QwLO5DYr5PV8POahs/bae639meXk7Z6bwxhjBAAAYJGIcE8AAADgdAQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBTgHLZ48WI5HA59/vnnYRn/zjvv1AUXXBCWsVsKh8OhvLy8cE8DsA4BBWgh5s2bJ4fDoSFDhoR7KgEOHTqkvLw87dixI2xz2Llzp2655RalpKQoJiZG559/vkaMGKFnn302bHMC0DAEFKCFWLJkiS644AJt3bpVe/fuDfd0/A4dOqTp06fXGVB++9vfas+ePU06/qZNmzRw4EB99NFHuueee/Tcc8/pJz/5iSIiIvSb3/ymSccG0HSiwj0BAN9t37592rRpk1599VXdd999WrJkiaZNmxbuaX0np9PZ5GP84he/UFxcnLZt26YOHToEbCstLW3y8QE0Da6gAC3AkiVLdN555ykrK0u33HKLlixZUqvP7t27NWzYMLVp00Zdu3bV448/rurq6jqPN2/ePPXu3Vsul0tJSUnKycnR0aNHA/pcffXV6tOnj4qKinTFFVeoTZs26t69u+bPn+/v895772nQoEGSpLvuuksOh0MOh0OLFy+WVPd7UE6cOKGf/exnSk5OlsvlUo8ePfTUU0/p9B9WdzgcmjhxolasWKE+ffrI5XKpd+/eWr16dUC/zz77TL17964VTiQpPj6+zmMuWbJEPXr0UExMjAYMGKCNGzfW2veLL77Q3XffrYSEBP/YL774Yq1+Xq9X06ZN08UXXyyXy6Xk5GQ9/PDD8nq9tfpNnjxZnTt3Vvv27XX99dfrn//8Z63jAfj/GQDW69mzpxk/frwxxpiNGzcaSWbr1q3+7YcPHzadO3c25513nsnLyzO/+tWvzCWXXGL69u1rJJl9+/b5+06bNs1IMunp6ebZZ581EydONJGRkWbQoEGmsrLS32/o0KEmKSnJxMfHm4kTJ5q5c+eaq666ykgyCxcuNMYYU1xcbGbMmGEkmXvvvde8/PLL5uWXXzafffaZMcaY7Oxsk5KS4j9mdXW1GTZsmHE4HOYnP/mJee6558yoUaOMJDNp0qSAc5ZkLr/8ctOlSxczc+ZM88wzz5gLL7zQxMbGmq+++srfLyMjw7Rv397s3LnzO+soyfTp08d873vfMzNmzDBPPvmkSUlJMW3atAnYv7i42HTt2tUkJyebGTNmmOeff95cf/31RpKZM2eOv19VVZXJyMgwsbGxZtKkSWbBggVm4sSJJioqytxwww0BY99xxx1Gkrn99tvNc889Z26++Wb/4zNt2rTvnDvQ2hBQAMtt377dSDIFBQXGmG//yHft2tU8+OCD/j6TJk0yksyWLVv8baWlpSYuLi4goJSWlpro6GiTkZFhqqqq/H2fe+45I8m8+OKL/rahQ4caSebXv/61v83r9Zp+/fqZ+Ph4f5jZtm2bkWQWLVpUa+6nB5QVK1YYSebxxx8P6HfLLbcYh8Nh9u7d62+TZKKjowPaPvroIyPJPPvss/62tWvXmsjISBMZGWk8Ho95+OGHzZo1awLC1n8eU5LZvn27v23//v0mJibG3HTTTf628ePHmy5dugQEIWOMGTNmjImLizMVFRXGGGNefvllExERYf7v//4voN/8+fONJPP+++8bY4zZsWOHkWR++tOfBvS7/fbbCSjAGfASD2C5JUuWKCEhQddcc42kb1+muPXWW7Vs2TJVVVVJklatWqW0tDQNHjzYv1/nzp01duzYgGOtW7dOlZWVmjRpkiIi/v2f/z333CO326233noroH9UVJTuu+8+//3o6Gjdd999Ki0tVVFRUcjnsmrVKkVGRuqBBx4IaP/Zz34mY4zefvvtgPb09HRddNFF/vt9+/aV2+3WP/7xD3/biBEjVFhYqOuvv14fffSRZs+erczMTJ1//vl64403as3B4/FowIAB/vvdunXTDTfcoDVr1qiqqkrGGP35z3/WqFGjZIzRV1995b9lZmaqrKxMH3zwgSRp+fLl6tWrl3r27BnQb9iwYZKkd99913/ekmqd96RJk0ItIdBqEFAAi1VVVWnZsmW65pprtG/fPu3du1d79+7VkCFDVFJSovXr10uS9u/fr0suuaTW/j169Ai4v3///jrbo6OjdeGFF/q310hKSlLbtm0D2i699FJJqtd3q+zfv19JSUlq3759QHuvXr0C5lejW7dutY5x3nnn6euvvw5oGzRokF599VV9/fXX2rp1q6ZMmaJjx47plltu0ccffxzQt646XXrppaqoqNCXX36pL7/8UkePHtULL7ygzp07B9zuuusuSf9+8+2nn36q3bt31+pXU6Oafvv371dERERA2JJqPw4A/o1P8QAWe+edd3T48GEtW7ZMy5Ytq7V9yZIlysjICMPMmkdkZGSd7ea0N9TWiI6O1qBBgzRo0CBdeumluuuuu7R8+fKQPvFU88biO+64Q9nZ2XX26du3r7/vZZddpqeffrrOfsnJyUGPCyAQAQWw2JIlSxQfH6/8/Pxa21599VW99tprmj9/vlJSUvTpp5/W6nP6d5CkpKT42y+88EJ/e2Vlpfbt26f09PSA/ocOHdKJEycCrqL8/e9/lyT/p3McDkfQ55OSkqJ169bp2LFjAVdR/va3vwXMrzEMHDhQknT48OGA9rrq9Pe//12xsbHq3LmzJKl9+/aqqqqqVY/TXXTRRfroo480fPjws9YhJSVF1dXV+uyzzwKumjT1d8QALRkv8QCW+uabb/Tqq6/qBz/4gW655ZZat4kTJ+rYsWN64403dN1112nz5s3aunWrf/8vv/yy1seR09PTFR0drblz5wZchVi4cKHKysqUlZUV0P/UqVNasGCB/35lZaUWLFigzp07+9/HURNeTv+Ycl2uu+46VVVV6bnnngtonzNnjhwOh0aOHBlccf7Du+++W+cVlZr3fZz+MkphYaH/PSSSdPDgQb3++uvKyMhQZGSkIiMjNXr0aP35z3/Wrl27ah33yy+/9P/7j370I33xxRf67W9/W6vfN998oxMnTkiS/7zmzp0b0OeZZ54J8iyB1ocrKICl3njjDR07dkzXX399ndvT0tLUuXNnLVmyRAsWLNDLL7+sa6+9Vg8++KDatm2rF154QSkpKfrrX//q36dz586aMmWKpk+frmuvvVbXX3+99uzZo3nz5mnQoEG64447AsZISkrSk08+qc8//1yXXnqp/vjHP2rHjh164YUX/F/CdtFFF6lDhw6aP3++2rdvr7Zt22rIkCHq3r17rTmPGjVK11xzjf73f/9Xn3/+uS6//HKtXbtWr7/+uiZNmlTrPRrBuP/++1VRUaGbbrpJPXv2VGVlpTZt2qQ//vGPuuCCC/zvG6nRp08fZWZm6oEHHpDL5dK8efMkSdOnT/f3eeKJJ/Tuu+9qyJAhuueee5SamqojR47ogw8+0Lp163TkyBFJ0rhx4/TKK6/ov/7rv/Tuu+/qyiuvVFVVlf72t7/plVde0Zo1azRw4ED169dPt912m+bNm6eysjJdccUVWr9+vVXfCAxYJ5wfIQJwZqNGjTIxMTHmxIkTZ+xz5513GqfTab766ivz17/+1QwdOtTExMSY888/38ycOdMsXLiw1vegGPPtx4p79uxpnE6nSUhIMBMmTDBff/11QJ+hQ4ea3r17m+3btxuPx2NiYmJMSkqKee6552rN4/XXXzepqakmKioq4CPHp3/M2Bhjjh07ZiZPnmySkpKM0+k0l1xyifnVr35lqqurA/pJMjk5ObXGSklJMdnZ2f77b7/9trn77rtNz549Tbt27Ux0dLS5+OKLzf33329KSkrqPOYf/vAHc8kllxiXy2X69+9v3n333VrjlJSUmJycHJOcnGycTqdJTEw0w4cPNy+88EJAv8rKSvPkk0+a3r17G5fLZc477zwzYMAAM336dFNWVubv980335gHHnjAdOrUybRt29aMGjXKHDx4kI8ZA2fgMOYM7zYD0KpdffXV+uqrr+p8maOlcjgcysnJqfUSEwD78B4UAABgHQIKAACwDgEFAABYh/egAAAA63AFBQAAWIeAAgAArNMiv6iturpahw4dUvv27UP6mm0AABA+xhgdO3ZMSUlJAb+oXpcWGVAOHTrEj3ABANBCHTx4UF27dj1rnxYZUGp+ZOzgwYNyu92Nemyfz6e1a9cqIyPD/1XeqBu1Ch61Ch61Ch61Ch61Ck1T1au8vFzJyckBPxZ6Ji0yoNS8rON2u5skoMTGxsrtdrOIvwO1Ch61Ch61Ch61Ch61Ck1T1yuYt2fwJlkAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA60SF0vmCCy7Q/v37a7X/9Kc/VX5+vk6ePKmf/exnWrZsmbxerzIzMzVv3jwlJCT4+x44cEATJkzQu+++q3bt2ik7O1uzZs1SVFRIUwEAoEW44NG3wj2FkLkijWYPDu8cQrqCsm3bNh0+fNh/KygokCT98Ic/lCRNnjxZb775ppYvX64NGzbo0KFDuvnmm/37V1VVKSsrS5WVldq0aZNeeuklLV68WFOnTm3EUwIAAC1dSAGlc+fOSkxM9N9Wrlypiy66SEOHDlVZWZkWLlyop59+WsOGDdOAAQO0aNEibdq0SZs3b5YkrV27Vh9//LH+8Ic/qF+/fho5cqRmzpyp/Px8VVZWNskJAgCAlqfer6tUVlbqD3/4g3Jzc+VwOFRUVCSfz6f09HR/n549e6pbt24qLCxUWlqaCgsLddlllwW85JOZmakJEyZo9+7d6t+/f51jeb1eeb1e//3y8nJJks/nk8/nq+8p1KnmeI193HMRtQoetQoetQoetQpeOGvlijTNPmZDuSK+nXNT/Y0NRr0DyooVK3T06FHdeeedkqTi4mJFR0erQ4cOAf0SEhJUXFzs7/Of4aRme822M5k1a5amT59eq33t2rWKjY2t7ymcVc3LV/hu1Cp41Cp41Cp41Cp44ahVuN/L0RCNXa+Kioqg+9Y7oCxcuFAjR45UUlJSfQ8RtClTpig3N9d/v7y8XMnJycrIyJDb7W7UsXw+nwoKCjRixAg5nc5GPfa5hloFj1oFj1oFj1oFL5y16pO3plnHawyuCKOZA6sbvV41r4AEo14BZf/+/Vq3bp1effVVf1tiYqIqKyt19OjRgKsoJSUlSkxM9PfZunVrwLFKSkr8287E5XLJ5XLVanc6nU220Jry2OcaahU8ahU8ahU8ahW8cNTKW+Vo1vEaU2PXK5Rj1et7UBYtWqT4+HhlZWX52wYMGCCn06n169f72/bs2aMDBw7I4/FIkjwej3bu3KnS0lJ/n4KCArndbqWmptZnKgAA4BwU8hWU6upqLVq0SNnZ2QHfXRIXF6fx48crNzdXHTt2lNvt1v333y+Px6O0tDRJUkZGhlJTUzVu3DjNnj1bxcXFeuyxx5STk1PnFRIAANA6hRxQ1q1bpwMHDujuu++utW3OnDmKiIjQ6NGjA76orUZkZKRWrlypCRMmyOPxqG3btsrOztaMGTMadhYAAOCcEnJAycjIkDF1f2QqJiZG+fn5ys/PP+P+KSkpWrVqVajDAgCAVoTf4gEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYJ+SA8sUXX+iOO+5Qp06d1KZNG1122WXavn27f7sxRlOnTlWXLl3Upk0bpaen69NPPw04xpEjRzR27Fi53W516NBB48eP1/Hjxxt+NgAA4JwQUkD5+uuvdeWVV8rpdOrtt9/Wxx9/rF//+tc677zz/H1mz56tuXPnav78+dqyZYvatm2rzMxMnTx50t9n7Nix2r17twoKCrRy5Upt3LhR9957b+OdFQAAaNGiQun85JNPKjk5WYsWLfK3de/e3f/vxhg988wzeuyxx3TDDTdIkn7/+98rISFBK1as0JgxY/TJJ59o9erV2rZtmwYOHChJevbZZ3XdddfpqaeeUlJSUmOcFwAAaMFCCihvvPGGMjMz9cMf/lAbNmzQ+eefr5/+9Ke65557JEn79u1TcXGx0tPT/fvExcVpyJAhKiws1JgxY1RYWKgOHTr4w4kkpaenKyIiQlu2bNFNN91Ua1yv1yuv1+u/X15eLkny+Xzy+XyhnfF3qDleYx/3XEStgketgketgketghfOWrkiTbOP2VCuiG/n3FR/Y4MRUkD5xz/+oeeff165ubn6n//5H23btk0PPPCAoqOjlZ2dreLiYklSQkJCwH4JCQn+bcXFxYqPjw+cRFSUOnbs6O9zulmzZmn69Om12teuXavY2NhQTiFoBQUFTXLccxG1Ch61Ch61Ch61Cl44ajV7cLMP2Wgau14VFRVB9w0poFRXV2vgwIH65S9/KUnq37+/du3apfnz5ys7Ozu0WYZgypQpys3N9d8vLy9XcnKyMjIy5Ha7G3Usn8+ngoICjRgxQk6ns1GPfa6hVsGjVsGjVsGjVsELZ6365K1p1vEagyvCaObA6kavV80rIMEIKaB06dJFqampAW29evXSn//8Z0lSYmKiJKmkpERdunTx9ykpKVG/fv38fUpLSwOOcerUKR05csS//+lcLpdcLletdqfT2WQLrSmPfa6hVsGjVsGjVsGjVsELR628VY5mHa8xNXa9QjlWSJ/iufLKK7Vnz56Atr///e9KSUmR9O0bZhMTE7V+/Xr/9vLycm3ZskUej0eS5PF4dPToURUVFfn7vPPOO6qurtaQIUNCmQ4AADhHhXQFZfLkybriiiv0y1/+Uj/60Y+0detWvfDCC3rhhRckSQ6HQ5MmTdLjjz+uSy65RN27d9fPf/5zJSUl6cYbb5T07RWXa6+9Vvfcc4/mz58vn8+niRMnasyYMXyCBwAASAoxoAwaNEivvfaapkyZohkzZqh79+565plnNHbsWH+fhx9+WCdOnNC9996ro0eP6qqrrtLq1asVExPj77NkyRJNnDhRw4cPV0REhEaPHq25c+c23lkBAIAWLaSAIkk/+MEP9IMf/OCM2x0Oh2bMmKEZM2acsU/Hjh21dOnSUIcGAACtBL/FAwAArBPyFRSgNbvg0bfqtZ8r0mj24G8/btjc7+j//ImsZh0PABoDV1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWCekgJKXlyeHwxFw69mzp3/7yZMnlZOTo06dOqldu3YaPXq0SkpKAo5x4MABZWVlKTY2VvHx8XrooYd06tSpxjkbAABwTogKdYfevXtr3bp1/z5A1L8PMXnyZL311ltavny54uLiNHHiRN188816//33JUlVVVXKyspSYmKiNm3apMOHD+vHP/6xnE6nfvnLXzbC6QAAgHNByAElKipKiYmJtdrLysq0cOFCLV26VMOGDZMkLVq0SL169dLmzZuVlpamtWvX6uOPP9a6deuUkJCgfv36aebMmXrkkUeUl5en6Ojohp8RAABo8UIOKJ9++qmSkpIUExMjj8ejWbNmqVu3bioqKpLP51N6erq/b8+ePdWtWzcVFhYqLS1NhYWFuuyyy5SQkODvk5mZqQkTJmj37t3q379/nWN6vV55vV7//fLyckmSz+eTz+cL9RTOquZ4jX3cc1FrrJUr0tRvvwgT8M/m1NIen9a4ruqLWgUvnLWq7/NGONU8VzXV39hgOIwxQVfu7bff1vHjx9WjRw8dPnxY06dP1xdffKFdu3bpzTff1F133RUQJCRp8ODBuuaaa/Tkk0/q3nvv1f79+7VmzRr/9oqKCrVt21arVq3SyJEj6xw3Ly9P06dPr9W+dOlSxcbGBjt9AAAQRhUVFbr99ttVVlYmt9t91r4hXUH5zwDRt29fDRkyRCkpKXrllVfUpk2b+s02CFOmTFFubq7/fnl5uZKTk5WRkfGdJxgqn8+ngoICjRgxQk6ns1GPfa5pjbXqk7fmuzvVwRVhNHNgtX6+PULeakcjz+rsduVlNut4DdUa11V9UavghbNW9X3eCKea56zGrlfNKyDBCPklnv/UoUMHXXrppdq7d69GjBihyspKHT16VB06dPD3KSkp8b9nJTExUVu3bg04Rs2nfOp6X0sNl8sll8tVq93pdDbZQmvKY59rWlOtvFUNCxfeakeDjxGqlvrYtKZ11VDUKnjhqFVz/zffmBq7XqEcq0Hfg3L8+HF99tln6tKliwYMGCCn06n169f7t+/Zs0cHDhyQx+ORJHk8Hu3cuVOlpaX+PgUFBXK73UpNTW3IVAAAwDkkpCso//3f/61Ro0YpJSVFhw4d0rRp0xQZGanbbrtNcXFxGj9+vHJzc9WxY0e53W7df//98ng8SktLkyRlZGQoNTVV48aN0+zZs1VcXKzHHntMOTk5dV4hAQAArVNIAeWf//ynbrvtNv3rX/9S586dddVVV2nz5s3q3LmzJGnOnDmKiIjQ6NGj5fV6lZmZqXnz5vn3j4yM1MqVKzVhwgR5PB61bdtW2dnZmjFjRuOeFQAAaNFCCijLli076/aYmBjl5+crPz//jH1SUlK0atWqUIYFAACtDL/FAwAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOgwLKE088IYfDoUmTJvnbTp48qZycHHXq1Ent2rXT6NGjVVJSErDfgQMHlJWVpdjYWMXHx+uhhx7SqVOnGjIVAABwDql3QNm2bZsWLFigvn37BrRPnjxZb775ppYvX64NGzbo0KFDuvnmm/3bq6qqlJWVpcrKSm3atEkvvfSSFi9erKlTp9b/LAAAwDklqj47HT9+XGPHjtVvf/tbPf744/72srIyLVy4UEuXLtWwYcMkSYsWLVKvXr20efNmpaWlae3atfr444+1bt06JSQkqF+/fpo5c6YeeeQR5eXlKTo6utZ4Xq9XXq/Xf7+8vFyS5PP55PP56nMKZ1RzvMY+7rmoNdbKFWnqt1+ECfhnc2ppj09rXFf1Ra2CF85a1fd5I5xqnqua6m9sMBzGmJArl52drY4dO2rOnDm6+uqr1a9fPz3zzDN65513NHz4cH399dfq0KGDv39KSoomTZqkyZMna+rUqXrjjTe0Y8cO//Z9+/bpwgsv1AcffKD+/fvXGi8vL0/Tp0+v1b506VLFxsaGOn0AABAGFRUVuv3221VWVia3233WviFfQVm2bJk++OADbdu2rda24uJiRUdHB4QTSUpISFBxcbG/T0JCQq3tNdvqMmXKFOXm5vrvl5eXKzk5WRkZGd95gqHy+XwqKCjQiBEj5HQ6G/XY55rWWKs+eWvqtZ8rwmjmwGr9fHuEvNWORp7V2e3Ky2zW8RqqNa6r+qJWwQtnrer7vBFONc9ZjV2vmldAghFSQDl48KAefPBBFRQUKCYmJuSJ1ZfL5ZLL5arV7nQ6m2yhNeWxzzWtqVbeqoaFC2+1o8HHCFVLfWxa07pqKGoVvHDUqrn/m29MjV2vUI4V0ptki4qKVFpaqu9///uKiopSVFSUNmzYoLlz5yoqKkoJCQmqrKzU0aNHA/YrKSlRYmKiJCkxMbHWp3pq7tf0AQAArVtIAWX48OHauXOnduzY4b8NHDhQY8eO9f+70+nU+vXr/fvs2bNHBw4ckMfjkSR5PB7t3LlTpaWl/j4FBQVyu91KTU1tpNMCAAAtWUgv8bRv3159+vQJaGvbtq06derkbx8/frxyc3PVsWNHud1u3X///fJ4PEpLS5MkZWRkKDU1VePGjdPs2bNVXFysxx57TDk5OXW+jAMAAFqfen3M+GzmzJmjiIgIjR49Wl6vV5mZmZo3b55/e2RkpFauXKkJEybI4/Gobdu2ys7O1owZMxp7KgAAoIVqcEB57733Au7HxMQoPz9f+fn5Z9wnJSVFq1ataujQAADgHMVv8QAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6UeGegK365K2Rt8oR7mkE7fMnssI9BQAAGg1XUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6IQWU559/Xn379pXb7Zbb7ZbH49Hbb7/t337y5Enl5OSoU6dOateunUaPHq2SkpKAYxw4cEBZWVmKjY1VfHy8HnroIZ06dapxzgYAAJwTQgooXbt21RNPPKGioiJt375dw4YN0w033KDdu3dLkiZPnqw333xTy5cv14YNG3To0CHdfPPN/v2rqqqUlZWlyspKbdq0SS+99JIWL16sqVOnNu5ZAQCAFi0qlM6jRo0KuP+LX/xCzz//vDZv3qyuXbtq4cKFWrp0qYYNGyZJWrRokXr16qXNmzcrLS1Na9eu1ccff6x169YpISFB/fr108yZM/XII48oLy9P0dHRjXdmAACgxQopoPynqqoqLV++XCdOnJDH41FRUZF8Pp/S09P9fXr27Klu3bqpsLBQaWlpKiws1GWXXaaEhAR/n8zMTE2YMEG7d+9W//796xzL6/XK6/X675eXl0uSfD6ffD5ffU+hTjXHc0WYRj1uU2vsOoQyZjjGDhdXZP3WRc16Cse6ammPT2tcV/VFrYIXzlrV93kjnGqeq5rqb2wwQg4oO3fulMfj0cmTJ9WuXTu99tprSk1N1Y4dOxQdHa0OHToE9E9ISFBxcbEkqbi4OCCc1Gyv2XYms2bN0vTp02u1r127VrGxsaGeQlBmDqxukuM2lVWrVoVt7IKCgrCN3dxmD27Y/uFYV+FcGw3RmtZVQ1Gr4IWjVg193ginxq5XRUVF0H1DDig9evTQjh07VFZWpj/96U/Kzs7Whg0bQj1MSKZMmaLc3Fz//fLyciUnJysjI0Nut7tRx/L5fCooKNDPt0fIW+1o1GM3pV15mc0+Zk2tRowYIafT2ezjh0OfvDX12s8VYTRzYHVY1lU41kZDtMZ1VV/UKnjhrFV9nzfCqeY5q7HrVfMKSDBCDijR0dG6+OKLJUkDBgzQtm3b9Jvf/Ea33nqrKisrdfTo0YCrKCUlJUpMTJQkJSYmauvWrQHHq/mUT02furhcLrlcrlrtTqezyRaat9ohb1XLCSjhfHJqysfBNg1dE+FYVy31sWlN66qhqFXwwlGrlvS35HSNXa9QjtXg70Gprq6W1+vVgAED5HQ6tX79ev+2PXv26MCBA/J4PJIkj8ejnTt3qrS01N+noKBAbrdbqampDZ0KAAA4R4R0BWXKlCkaOXKkunXrpmPHjmnp0qV67733tGbNGsXFxWn8+PHKzc1Vx44d5Xa7df/998vj8SgtLU2SlJGRodTUVI0bN06zZ89WcXGxHnvsMeXk5NR5hQQAALROIQWU0tJS/fjHP9bhw4cVFxenvn37as2aNRoxYoQkac6cOYqIiNDo0aPl9XqVmZmpefPm+fePjIzUypUrNWHCBHk8HrVt21bZ2dmaMWNG454VAABo0UIKKAsXLjzr9piYGOXn5ys/P/+MfVJSUlrspwoAAEDz4Ld4AACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOtEhXsCAHAuuODRt5p9TFek0ezBUp+8NfJWOULe//MnsppgVkDj4AoKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdUIKKLNmzdKgQYPUvn17xcfH68Ybb9SePXsC+pw8eVI5OTnq1KmT2rVrp9GjR6ukpCSgz4EDB5SVlaXY2FjFx8froYce0qlTpxp+NgAA4JwQUkDZsGGDcnJytHnzZhUUFMjn8ykjI0MnTpzw95k8ebLefPNNLV++XBs2bNChQ4d08803+7dXVVUpKytLlZWV2rRpk1566SUtXrxYU6dObbyzAgAALVpUKJ1Xr14dcH/x4sWKj49XUVGR/t//+38qKyvTwoULtXTpUg0bNkyStGjRIvXq1UubN29WWlqa1q5dq48//ljr1q1TQkKC+vXrp5kzZ+qRRx5RXl6eoqOjG+/sAABAixRSQDldWVmZJKljx46SpKKiIvl8PqWnp/v79OzZU926dVNhYaHS0tJUWFioyy67TAkJCf4+mZmZmjBhgnbv3q3+/fvXGsfr9crr9frvl5eXS5J8Pp98Pl9DTqGWmuO5IkyjHrepNXYdQhkzHGOHiyuyfuuiZj2FY121tMenpa6r+q6NBo3ZwHXV0mrcEOFcV+FYGw1Vs6aa6m9sMBzGmHpVrrq6Wtdff72OHj2qv/zlL5KkpUuX6q677goIE5I0ePBgXXPNNXryySd17733av/+/VqzZo1/e0VFhdq2batVq1Zp5MiRtcbKy8vT9OnTa7UvXbpUsbGx9Zk+AABoZhUVFbr99ttVVlYmt9t91r71voKSk5OjXbt2+cNJU5oyZYpyc3P998vLy5WcnKyMjIzvPMFQ+Xw+FRQU6OfbI+StdjTqsZvSrrzMZh+zplYjRoyQ0+ls9vHDoU/emu/uVAdXhNHMgdVhWVfhWBsN0VLXVX3XRkM0dF21tLXREOFcV+FYGw1Vs7Yau141r4AEo14BZeLEiVq5cqU2btyorl27+tsTExNVWVmpo0ePqkOHDv72kpISJSYm+vts3bo14Hg1n/Kp6XM6l8sll8tVq93pdDbZQvNWO+StajkBJZxP5E35ONimoWsiHOuqpT42LW1dhfP5or7rqiXVt7GEY121pL8lp2vseoVyrJA+xWOM0cSJE/Xaa6/pnXfeUffu3QO2DxgwQE6nU+vXr/e37dmzRwcOHJDH45EkeTwe7dy5U6Wlpf4+BQUFcrvdSk1NDWU6AADgHBXSFZScnBwtXbpUr7/+utq3b6/i4mJJUlxcnNq0aaO4uDiNHz9eubm56tixo9xut+6//355PB6lpaVJkjIyMpSamqpx48Zp9uzZKi4u1mOPPaacnJw6r5IAAIDWJ6SA8vzzz0uSrr766oD2RYsW6c4775QkzZkzRxERERo9erS8Xq8yMzM1b948f9/IyEitXLlSEyZMkMfjUdu2bZWdna0ZM2Y07EwAAMA5I6SAEswHfmJiYpSfn6/8/Pwz9klJSdGqVatCGRoAALQi/BYPAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDohB5SNGzdq1KhRSkpKksPh0IoVKwK2G2M0depUdenSRW3atFF6ero+/fTTgD5HjhzR2LFj5Xa71aFDB40fP17Hjx9v0IkAAIBzR8gB5cSJE7r88suVn59f5/bZs2dr7ty5mj9/vrZs2aK2bdsqMzNTJ0+e9PcZO3asdu/erYKCAq1cuVIbN27UvffeW/+zAAAA55SoUHcYOXKkRo4cWec2Y4yeeeYZPfbYY7rhhhskSb///e+VkJCgFStWaMyYMfrkk0+0evVqbdu2TQMHDpQkPfvss7ruuuv01FNPKSkpqQGnAwAAzgUhB5Sz2bdvn4qLi5Wenu5vi4uL05AhQ1RYWKgxY8aosLBQHTp08IcTSUpPT1dERIS2bNmim266qdZxvV6vvF6v/355ebkkyefzyefzNeYp+I/nijCNetym1th1CGXMcIwdLq7I+q2LmvUUjnXV0h6flrqu6rs2GjRmA9dVS6txQ4RzXYVjbTRUzZpqqr+xwWjUgFJcXCxJSkhICGhPSEjwbysuLlZ8fHzgJKKi1LFjR3+f082aNUvTp0+v1b527VrFxsY2xtRrmTmwukmO21RWrVoVtrELCgrCNnZzmz24YfuHY12Fc200REtbVw1dGw1R33XVUtdGQ4RjXYVzbTRUY9eroqIi6L6NGlCaypQpU5Sbm+u/X15eruTkZGVkZMjtdjfqWD6fTwUFBfr59gh5qx2NeuymtCsvs9nHrKnViBEj5HQ6m338cOiTt6Ze+7kijGYOrA7LugrH2miIlrqu6rs2GqKh66qlrY2GCOe6CsfaaKiatdXY9ap5BSQYjRpQEhMTJUklJSXq0qWLv72kpET9+vXz9yktLQ3Y79SpUzpy5Ih//9O5XC65XK5a7U6ns8kWmrfaIW9Vywko4Xwib8rHwTYNXRPhWFct9bFpaesqnM8X9V1XLam+jSUc66ol/S05XWPXK5RjNer3oHTv3l2JiYlav369v628vFxbtmyRx+ORJHk8Hh09elRFRUX+Pu+8846qq6s1ZMiQxpwOAABooUK+gnL8+HHt3bvXf3/fvn3asWOHOnbsqG7dumnSpEl6/PHHdckll6h79+76+c9/rqSkJN14442SpF69eunaa6/VPffco/nz58vn82nixIkaM2YMn+ABAACS6hFQtm/frmuuucZ/v+a9IdnZ2Vq8eLEefvhhnThxQvfee6+OHj2qq666SqtXr1ZMTIx/nyVLlmjixIkaPny4IiIiNHr0aM2dO7cRTgcAAJwLQg4oV199tYw580emHA6HZsyYoRkzZpyxT8eOHbV06dJQhwYAAK0Ev8UDAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ2wBpT8/HxdcMEFiomJ0ZAhQ7R169ZwTgcAAFgibAHlj3/8o3JzczVt2jR98MEHuvzyy5WZmanS0tJwTQkAAFgibAHl6aef1j333KO77rpLqampmj9/vmJjY/Xiiy+Ga0oAAMASUeEYtLKyUkVFRZoyZYq/LSIiQunp6SosLKzV3+v1yuv1+u+XlZVJko4cOSKfz9eoc/P5fKqoqFCUL0JV1Y5GPXZT+te//tXsY9bU6l//+pecTmezjx8OUadO1G+/aqOKiuqwrKtwrI2GaKnrqr5ro0FjNnBdtbS10RDhXFfhWBsNVbO2Grtex44dkyQZY767swmDL774wkgymzZtCmh/6KGHzODBg2v1nzZtmpHEjRs3bty4cTsHbgcPHvzOrBCWKyihmjJlinJzc/33q6urdeTIEXXq1EkOR+P+32h5ebmSk5N18OBBud3uRj32uYZaBY9aBY9aBY9aBY9ahaap6mWM0bFjx5SUlPSdfcMSUL73ve8pMjJSJSUlAe0lJSVKTEys1d/lcsnlcgW0dejQoSmnKLfbzSIOErUKHrUKHrUKHrUKHrUKTVPUKy4uLqh+YXmTbHR0tAYMGKD169f726qrq7V+/Xp5PJ5wTAkAAFgkbC/x5ObmKjs7WwMHDtTgwYP1zDPP6MSJE7rrrrvCNSUAAGCJsAWUW2+9VV9++aWmTp2q4uJi9evXT6tXr1ZCQkK4piTp25eTpk2bVuslJdRGrYJHrYJHrYJHrYJHrUJjQ70cxgTzWR8AAIDmw2/xAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwTqsKKBs3btSoUaOUlJQkh8OhFStWfOc+7733nr7//e/L5XLp4osv1uLFi5t8njYItVbvvfeeHA5HrVtxcXHzTDiMZs2apUGDBql9+/aKj4/XjTfeqD179nznfsuXL1fPnj0VExOjyy67TKtWrWqG2YZXfWq1ePHiWusqJiammWYcPs8//7z69u3r/yZPj8ejt99++6z7tMY1VSPUerXWdXW6J554Qg6HQ5MmTTprv3CsrVYVUE6cOKHLL79c+fn5QfXft2+fsrKydM0112jHjh2aNGmSfvKTn2jNmjVNPNPwC7VWNfbs2aPDhw/7b/Hx8U00Q3ts2LBBOTk52rx5swoKCuTz+ZSRkaETJ878C6abNm3SbbfdpvHjx+vDDz/UjTfeqBtvvFG7du1qxpk3v/rUSvr267b/c13t37+/mWYcPl27dtUTTzyhoqIibd++XcOGDdMNN9yg3bt319m/ta6pGqHWS2qd6+o/bdu2TQsWLFDfvn3P2i9sa6txfp+45ZFkXnvttbP2efjhh03v3r0D2m699VaTmZnZhDOzTzC1evfdd40k8/XXXzfLnGxWWlpqJJkNGzacsc+PfvQjk5WVFdA2ZMgQc9999zX19KwSTK0WLVpk4uLimm9SFjvvvPPM7373uzq3saZqO1u9Wvu6OnbsmLnkkktMQUGBGTp0qHnwwQfP2Ddca6tVXUEJVWFhodLT0wPaMjMzVVhYGKYZ2a9fv37q0qWLRowYoffffz/c0wmLsrIySVLHjh3P2Ie19a1gaiVJx48fV0pKipKTk7/z/4rPRVVVVVq2bJlOnDhxxt8rY039WzD1klr3usrJyVFWVlatNVOXcK2tsH3VfUtQXFxc66v3ExISVF5erm+++UZt2rQJ08zs06VLF82fP18DBw6U1+vV7373O1199dXasmWLvv/974d7es2murpakyZN0pVXXqk+ffqcsd+Z1lZreM9OjWBr1aNHD7344ovq27evysrK9NRTT+mKK67Q7t271bVr12accfPbuXOnPB6PTp48qXbt2um1115TampqnX1ZU6HVqzWvq2XLlumDDz7Qtm3bguofrrVFQEGj6NGjh3r06OG/f8UVV+izzz7TnDlz9PLLL4dxZs0rJydHu3bt0l/+8pdwT8V6wdbK4/EE/F/wFVdcoV69emnBggWaOXNmU08zrHr06KEdO3aorKxMf/rTn5Sdna0NGzac8Y9uaxdKvVrrujp48KAefPBBFRQUWP+mYALKWSQmJqqkpCSgraSkRG63m6snQRg8eHCr+kM9ceJErVy5Uhs3bvzO/wM709pKTExsyilaI5Ranc7pdKp///7au3dvE83OHtHR0br44oslSQMGDNC2bdv0m9/8RgsWLKjVt7WvKSm0ep2utayroqIilZaWBlzZrqqq0saNG/Xcc8/J6/UqMjIyYJ9wrS3eg3IWHo9H69evD2grKCg462ua+LcdO3aoS5cu4Z5GkzPGaOLEiXrttdf0zjvvqHv37t+5T2tdW/Wp1emqqqq0c+fOVrG2TlddXS2v11vntta6ps7mbPU6XWtZV8OHD9fOnTu1Y8cO/23gwIEaO3asduzYUSucSGFcW036FlzLHDt2zHz44Yfmww8/NJLM008/bT788EOzf/9+Y4wxjz76qBk3bpy//z/+8Q8TGxtrHnroIfPJJ5+Y/Px8ExkZaVavXh2uU2g2odZqzpw5ZsWKFebTTz81O3fuNA8++KCJiIgw69atC9cpNJsJEyaYuLg4895775nDhw/7bxUVFf4+48aNM48++qj//vvvv2+ioqLMU089ZT755BMzbdo043Q6zc6dO8NxCs2mPrWaPn26WbNmjfnss89MUVGRGTNmjImJiTG7d+8Oxyk0m0cffdRs2LDB7Nu3z/z1r381jz76qHE4HGbt2rXGGNbU6UKtV2tdV3U5/VM8tqytVhVQaj4Ke/otOzvbGGNMdna2GTp0aK19+vXrZ6Kjo82FF15oFi1a1OzzDodQa/Xkk0+aiy66yMTExJiOHTuaq6++2rzzzjvhmXwzq6tOkgLWytChQ/21q/HKK6+YSy+91ERHR5vevXubt956q3knHgb1qdWkSZNMt27dTHR0tElISDDXXXed+eCDD5p/8s3s7rvvNikpKSY6Otp07tzZDB8+3P/H1hjW1OlCrVdrXVd1OT2g2LK2HMYY07TXaAAAAELDe1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ3/DwEixP9eHv/7AAAAAElFTkSuQmCC"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"final_submission.to_csv(os.path.join(CFG.OUTPUT_DIR, 'submission.csv'), index=False)\n\nprint(\"Submission file saved at:\", os.path.join(CFG.OUTPUT_DIR, 'submission.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-21T22:25:17.255859Z","iopub.execute_input":"2024-10-21T22:25:17.256286Z","iopub.status.idle":"2024-10-21T22:25:17.294268Z","shell.execute_reply.started":"2024-10-21T22:25:17.256246Z","shell.execute_reply":"2024-10-21T22:25:17.292942Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinal_submission\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG\u001b[38;5;241m.\u001b[39mOUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmission file saved at:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG\u001b[38;5;241m.\u001b[39mOUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'final_submission' is not defined"],"ename":"NameError","evalue":"name 'final_submission' is not defined","output_type":"error"}],"execution_count":23}]}