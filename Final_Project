{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86259,"databundleVersionId":9778966,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Library imports","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install torch\n# !pip install --upgrade nltk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standard library imports\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\nimport gc\nimport datetime\nimport re\nimport string\nimport itertools\nimport shutil\nimport glob\nimport time\nfrom tempfile import TemporaryDirectory\nfrom collections import defaultdict, Counter\nimport warnings\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms, models\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision.utils import make_grid\nfrom sklearn.metrics import (\n    confusion_matrix,\n    f1_score,\n    balanced_accuracy_score,\n    classification_report,\n    roc_auc_score,\n    accuracy_score\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom tqdm.auto import tqdm\n# import nltk\n# from nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\n# from nltk.stem.porter import PorterStemmer\nimport spacy\nimport gensim\nfrom gensim.models import word2vec, KeyedVectors\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom transformers import RobertaTokenizer, RobertaModel\n\n# Local application imports\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data import","metadata":{}},{"cell_type":"code","source":"class CFG:\n    BASE_DIR = \"/kaggle/input/deep-learning-for-computer-vision-and-nlp-2024-10/\"\n    TRAIN_DIR = \"/kaggle/input/deep-learning-for-computer-vision-and-nlp-2024-10/train\"\n    TEST_DIR = \"/kaggle/input/deep-learning-for-computer-vision-and-nlp-2024-10/test\"\n    \n    OUTPUT_DIR = '/kaggle/working/'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(f\"{ CFG.TRAIN_DIR }.csv\", sep=\",\", dtype=str, keep_default_na=False) #index_col='PetID'\n\ndf_test = pd.read_csv(f\"{ CFG.TEST_DIR }.csv\", sep=\",\", dtype=str, keep_default_na=False) #index_col='PetID'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['AdoptionSpeed'] = pd.to_numeric(df['AdoptionSpeed'])\ndf['PetID'] = df['PetID'].astype('str')\nprint(df)\nprint(df.info())\ndf.hist('AdoptionSpeed')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Image Info analysis","metadata":{}},{"cell_type":"code","source":"# Define the folder path where your images are located\nimages_folder = CFG.BASE_DIR + \"images/images/train\"\n\nimage_filenames = os.listdir(images_folder)\n\n# Initialize a dictionary to hold grouped images\nid_to_images = defaultdict(list)\n\nfor filename in image_filenames:\n    if filename.endswith(('.jpg', '.png', '.jpeg')):\n        # Extract the ID from the filename\n        id_part = filename.split('-')[0]  # Assuming format is '{id}-{image_number}.jpg'\n        id_to_images[id_part].append(os.path.join(images_folder, filename))\n\nimages_df = pd.DataFrame(columns=[\"PetID\", \"Count_Images\"])     \n        \n# Display the grouped images\nfor group, files in id_to_images.items():\n#     print(f\"Group '{group}' contains {len(files)} images:\")\n    new_row = {\"PetID\": group, \"Count_Images\": len(files)}\n    images_df = pd.concat([images_df, pd.DataFrame([new_row])], ignore_index=True)\n\n    \n#     for file in files:\n#         print(f\" - {file}\")\n\n# print(pd.DataFrame(grouped_images.keys(), ))\n\n# for id in grouped_images.keys():\n#     print(df['PetID'].equals(id))\n# #     print(df.loc[df['PetID'] == id])\nimages_df.sort_values('PetID', ascending=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_df.loc[images_df['PetID'] == \"000a290e4\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.loc[df['PetID'] == \"000a290e4\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.merge(df, images_df, on='PetID')       \n\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom collections import defaultdict\n\n# Path to the images folder\nimages_folder = CFG.BASE_DIR + \"images/images/train\"\n\n# List all image filenames\nimage_filenames = os.listdir(images_folder)\n\n# Dictionary to map IDs to image file paths\nid_to_images = defaultdict(list)\n\nfor filename in image_filenames:\n    if filename.endswith(('.jpg', '.png', '.jpeg')):\n        # Extract the ID from the filename\n        id_part = filename.split('-')[0]  # Assuming format is '{id}-{image_number}.jpg'\n        id_to_images[id_part].append(os.path.join(images_folder, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Text Analysis","metadata":{}},{"cell_type":"markdown","source":"### Creating embeddings from pretrained embedding models","metadata":{}},{"cell_type":"code","source":"for i in df.columns:\n    print(f'unique in column {i} -> {len(df[i].unique())}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.groupby('Description').count().sort_values('PetID', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"description_grouping = df.groupby('Description').count().sort_values('AdoptionSpeed', ascending=False)\n\ndescription_grouping.loc[description_grouping['AdoptionSpeed'] > 1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = 'For Adoption'\ndf.loc[df['Description'] == text].hist('AdoptionSpeed')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Removing Duplicates","metadata":{}},{"cell_type":"code","source":"for i in df.columns:\n    print(f'unique in column {i} -> {len(df[i].unique())}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop_duplicates().reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"duplicated = df[df['Description'].duplicated()]['Description'].unique()\n\nprint(f'num duplicates -> {len(duplicated)}\\n {pd.DataFrame(duplicated)}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_dedupe_text = df[~df['Description'].isin(duplicated)].reset_index(drop=True)\n\ndf_dedupe_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the frequency of each value for both datasets\ncounter_1 = Counter(df['AdoptionSpeed'])\ncounter_2 = Counter(df_dedupe_text['AdoptionSpeed'])\n\n# Sort the data based on the x-axis (values)\nsorted_data_1 = sorted(counter_1.items())  # List of tuples (value, count)\nsorted_data_2 = sorted(counter_2.items())  # List of tuples (value, count)\n\n# Unpack the sorted values and counts\nx_values_1, y_counts_1 = zip(*sorted_data_1)\nx_values_2, y_counts_2 = zip(*sorted_data_2)\n\n# Plotting the first dataset\nplt.bar(x_values_1, y_counts_1, width=0.4, color='blue', align='center', label='Original')\n\n# Plotting the second dataset with a slight shift on the x-axis for better visibility\nplt.bar([x for x in x_values_2], y_counts_2, width=0.4, color='orange', align='center', label='Without Outliers')\n\n# Adding labels, legend, and title\nplt.xlabel('Values')\nplt.ylabel('Count of Instances')\nplt.title('Distribution of AdoptionSpeed')\nplt.legend()\n\n# Display the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RoBERTa Model","metadata":{}},{"cell_type":"markdown","source":"### Loading RoBERTa model and Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel\nimport torch\n\n# Load the pre-trained RoBERTa tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Load the pre-trained RoBERTa model\nmodel = RobertaModel.from_pretrained('roberta-base')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing text data","metadata":{}},{"cell_type":"code","source":"texts = list(df['Description'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_inputs = tokenizer(\n    texts,\n    padding=True,\n    truncation=True,\n    max_length=256,  # Reduce from 512 to 256 or 128\n    return_tensors='pt'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids = encoded_inputs['input_ids'].to(device)\nattention_mask = encoded_inputs['attention_mask'].to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Generating Text Embeddings with RoBERTa","metadata":{}},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()  # Set to evaluation mode\n\nbatch_size = 8  # Start with 1\nmax_length = 256  # Adjust based on your data\n\nall_embeddings = []\nwith torch.no_grad():\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        encoded_inputs = tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            max_length=max_length,\n            return_tensors='pt'\n        )\n        input_ids = encoded_inputs['input_ids'].to(device)\n        attention_mask = encoded_inputs['attention_mask'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n\n        # Move embeddings to CPU and convert to numpy if necessary\n        embeddings = embeddings.cpu().numpy()\n        all_embeddings.append(embeddings)\n\n        # Clean up\n        del input_ids, attention_mask, outputs, embeddings\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Adjusting model architecture","metadata":{"execution":{"iopub.status.busy":"2024-10-20T22:02:03.232894Z","iopub.execute_input":"2024-10-20T22:02:03.233800Z"}}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass TextModel(nn.Module):\n    def __init__(self):\n        super(TextModel, self).__init__()\n        self.roberta = RobertaModel.from_pretrained('distilroberta-base')\n        self.dropout = nn.Dropout(p=0.3)\n        self.output_dim = 256\n        self.fc = nn.Linear(self.roberta.config.hidden_size, self.output_dim)\n        \n        # Freeze layers if necessary\n        for param in self.roberta.embeddings.parameters():\n            param.requires_grad = False\n        for param in self.roberta.encoder.layer[:2].parameters():  # Freeze first 2 layers\n            param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        x = outputs.last_hidden_state[:, 0, :]\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\n\nclass ImageModel(nn.Module):\n    def __init__(self):\n        super(ImageModel, self).__init__()\n        self.model = models.resnet18(pretrained=True)\n        self.model.fc = nn.Identity()\n        self.output_dim = 512  # ResNet18's output dimension\n        \n        # Freeze layers if necessary\n        for param in self.model.parameters():\n            param.requires_grad = False\n        for param in self.model.layer4.parameters():  # Unfreeze last block\n            param.requires_grad = True\n\n    def forward(self, images):\n        # images shape: [batch_size, max_num_images, channels, height, width]\n        batch_size, num_images, channels, height, width = images.shape\n        images = images.view(-1, channels, height, width)  # Reshape to process all images\n        features = self.model(images)  # Shape: [batch_size * num_images, feature_dim]\n        # Reshape back to [batch_size, num_images, feature_dim]\n        features = features.view(batch_size, num_images, -1)\n        # Aggregate features, e.g., by taking the mean\n        features = torch.mean(features, dim=1)  # Shape: [batch_size, feature_dim]\n        return features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training the updated model","metadata":{}},{"cell_type":"markdown","source":"#### Preparing DataLoaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MultimodalDataset(Dataset):\n    def __init__(self, text_data, labels=None, id_to_images=None, tokenizer=None, transform=None, max_length=256):\n        self.ids = list(text_data.keys())\n        self.text_data = text_data\n        self.labels = labels  # Labels can be None\n        self.id_to_images = id_to_images if id_to_images is not None else {}\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        id_ = self.ids[idx]\n        text = self.text_data[id_]\n\n        # Tokenize text\n        encoded = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        input_ids = encoded['input_ids'].squeeze(0)\n        attention_mask = encoded['attention_mask'].squeeze(0)\n\n        # Load images associated with the ID\n        image_paths = self.id_to_images.get(id_, [])\n        images = []\n        for image_path in image_paths:\n            image = Image.open(image_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            images.append(image)\n        if images:\n            images = torch.stack(images)\n        else:\n            # Handle case where no images are found\n            # Create a dummy image tensor filled with zeros\n            image_size = self.transform.transforms[0].size[0] if self.transform else 224\n            images = torch.zeros((1, 3, image_size, image_size))\n\n        # Prepare the sample dictionary\n        sample = {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'images': images,\n            'id': id_\n        }\n\n        # Include labels only if they are available\n        if self.labels is not None:\n            label = self.labels[id_]\n            label = torch.tensor(label, dtype=torch.long)\n            sample['label'] = label\n\n        return sample","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Data Transformations and Preprocessors","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\nimage_size = 224  # Adjust based on your model's requirements\n\nimage_transforms = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    # Normalize with mean and std of ImageNet if using pre-trained models\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Testing the dataset","metadata":{}},{"cell_type":"code","source":"data_df = df\n\ntext_data = dict(zip(data_df['PetID'].astype(str), data_df['Description']))\nlabels = dict(zip(data_df['PetID'].astype(str), data_df['AdoptionSpeed']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = MultimodalDataset(\n    text_data=text_data,\n    labels=labels,\n    id_to_images=id_to_images,\n    tokenizer=tokenizer,\n    transform=image_transforms,\n    max_length=256\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = dataset[0]\nprint(\"ID:\", sample['id'])\nprint(\"Input IDs shape:\", sample['input_ids'].shape)\nprint(\"Attention mask shape:\", sample['attention_mask'].shape)\nprint(\"Images shape:\", sample['images'].shape)\nprint(\"Label:\", sample['label'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Adjusting dataloader for variable image counts","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = torch.stack([item['input_ids'] for item in batch])\n    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n\n    # Handle images\n    images = [item['images'] for item in batch]\n    max_num_images = max([img.shape[0] for img in images])\n    padded_images = []\n    for img in images:\n        num_images = img.shape[0]\n        if num_images < max_num_images:\n            # Pad with zeros\n            padding = torch.zeros((max_num_images - num_images, *img.shape[1:]))\n            img = torch.cat([img, padding], dim=0)\n        padded_images.append(img)\n    images = torch.stack(padded_images)\n\n    ids = [item['id'] for item in batch]\n\n    batch_dict = {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'images': images,\n        'id': ids\n    }\n\n    # Include labels if they are present\n    if 'label' in batch[0]:\n        labels = torch.stack([item['label'] for item in batch])\n        batch_dict['label'] = labels\n\n    return batch_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\nlabels_list = [label for label in train_dataset.labels.values()]\nlabels_list = [label - 1 for label in labels_list]  # Adjust if labels start from 1\nlabel_counts = Counter(labels_list)\nprint(\"Training Data Class Distribution:\", label_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Assuming label_counts is obtained from the Counter\ntotal_samples = sum(label_counts.values())\nclass_weights = []\nfor i in range(num_classes):\n    class_count = label_counts.get(i, 0)\n    weight = total_samples / (num_classes * class_count)\n    class_weights.append(weight)\n\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=4,  # Adjust based on memory constraints\n    shuffle=True,\n    collate_fn=collate_fn\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Combining the text and image models","metadata":{}},{"cell_type":"code","source":"text_feature_dim = 256\nimage_feature_dim = 2048\n\nclass MultimodalModel(nn.Module):\n    def __init__(self, text_model, image_model, num_classes):\n        super(MultimodalModel, self).__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        combined_feature_dim = self.text_model.output_dim + self.image_model.output_dim\n        self.classifier = nn.Sequential(\n            nn.Linear(combined_feature_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        text_features = self.text_model(input_ids, attention_mask)\n        image_features = self.image_model(images)\n        combined = torch.cat((text_features, image_features), dim=1)\n        output = self.classifier(combined)\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Trinaing the model","metadata":{}},{"cell_type":"code","source":"labels_list = [label for label in train_dataset.labels.values()]\nlabels_list = [label - 1 for label in labels_list]  # Adjust if labels start from 1\nlabel_counts = Counter(labels_list)\nprint(\"Training Data Class Distribution:\", label_counts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Labels:\", labels)\n# print(\"Labels Shape:\", labels.shape)\n# print(\"Labels Min:\", labels.min().item())\n# print(\"Labels Max:\", labels.max().item())\n# print(\"Labels Data Type:\", labels.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.cuda.amp import autocast, GradScaler\n\n# Initialize models\ntext_model = TextModel().to(device)\nimage_model = ImageModel().to(device)\nmodel = MultimodalModel(text_model, image_model, num_classes=4).to(device)\n\n# Optimizer and loss function\nscaler = GradScaler()\n\nnum_epochs = 3\n\n# Training loop\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n\nbest_val_accuracy = 0.0\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        images = batch['images'].to(device)\n        labels = batch['label'].to(device)\n        labels = labels - 1  # Adjust labels\n\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\")\n\n    # Evaluate on validation set\n    val_accuracy, _, _, _, _ = evaluate(model, val_dataloader)\n    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n\n    # Adjust learning rate\n    scheduler.step(avg_train_loss)\n\n    # Save the best model\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(\"Best model saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming 'dataset' is your entire dataset\ntrain_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=16,  # Adjust based on memory constraints\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=4  # Adjust based on your system\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n\ndef evaluate(model, dataloader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['images'].to(device)\n            labels = batch['label'].to(device)\n\n            # Adjust labels if necessary\n            labels = labels - 1  # Zero-based indexing\n            labels = labels.long()\n\n            outputs = model(input_ids, attention_mask, images)\n            _, preds = torch.max(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='weighted'\n    )\n    cm = confusion_matrix(all_labels, all_preds)\n\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n    print(\"Confusion Matrix:\")\n    print(cm)\n\n    return accuracy, precision, recall, f1, cm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for epoch in range(num_epochs):\n#     model.train()\n#     for batch_idx, batch in enumerate(dataloader):\n#         optimizer.zero_grad()\n        \n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         images = batch['images'].to(device)\n#         labels = batch['label'].to(device)\n        \n#         # Adjust labels to zero-based indexing if necessary\n#         labels = labels - 1  # Only if your labels start from 1\n#         labels = labels.long()\n        \n#         # Debugging: print labels\n#         if batch_idx == 0 and epoch == 0:\n#             print(\"Labels:\", labels)\n#             print(\"Labels Shape:\", labels.shape)\n#             print(\"Labels Min:\", labels.min().item())\n#             print(\"Labels Max:\", labels.max().item())\n#             print(\"Labels Data Type:\", labels.dtype)\n        \n#         with autocast():\n#             outputs = model(input_ids, attention_mask, images)\n#             loss = criterion(outputs, labels)\n        \n#         scaler.scale(loss).backward()\n#         scaler.step(optimizer)\n#         scaler.update()\n\n#     # Optionally, add validation logic\n#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n#     evaluate(model, val_dataloader)\n\n#     batch = next(iter(dataloader))\n#     labels = batch['label']\n    \n#     print(\"Labels:\", labels)\n#     print(\"Labels Shape:\", labels.shape)\n#     print(\"Labels Data Type:\", labels.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# # Assume you store accuracy and loss in lists\n# train_losses = []\n# val_accuracies = []\n\n# # In your training loop\n# for epoch in range(num_epochs):\n#     # Training code...\n#     train_loss = ...\n#     train_losses.append(train_loss)\n\n#     # Evaluation\n#     accuracy, _, _, _, _ = evaluate(model, val_dataloader)\n#     val_accuracies.append(accuracy)\n\n# # Plotting\n# plt.figure(figsize=(10, 5))\n# plt.subplot(1, 2, 1)\n# plt.plot(train_losses, label='Training Loss')\n# plt.legend()\n# plt.subplot(1, 2, 2)\n# plt.plot(val_accuracies, label='Validation Accuracy')\n# plt.legend()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Dataloaders","metadata":{}},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n\n# dataset = MultimodalDataset(texts, image_paths, labels, tokenizer, transform=image_transforms)\n# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training the Word2Vec model","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom collections import defaultdict\n\n# Path to the images folder\ntest_images_folder = CFG.BASE_DIR + \"images/images/test\"\n\n# List all image filenames\ntest_image_filenames = os.listdir(test_images_folder)\n\n# Dictionary to map IDs to image file paths\ntest_id_to_images = defaultdict(list)\n\nfor filename in test_image_filenames:\n    if filename.endswith(('.jpg', '.png', '.jpeg')):\n        # Extract the ID from the filename\n        id_part = filename.split('-')[0]  # Assuming format is '{id}-{image_number}.jpg'\n        test_id_to_images[id_part].append(os.path.join(test_images_folder, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_text_data = df_test['Description']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ntest_dataset = MultimodalDataset(\n    text_data=test_text_data,\n    labels=None,  # No labels provided\n    id_to_images=test_id_to_images,\n    tokenizer=tokenizer,\n    transform=image_transforms,\n    max_length=256\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=16,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=4,\n    pin_memory=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(model, dataloader):\n    model.eval()\n    predictions = {}\n    with torch.no_grad():\n        for batch in dataloader:\n            ids = batch['id']\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['images'].to(device)\n\n            outputs = model(input_ids, attention_mask, images)\n            _, preds = torch.max(outputs, dim=1)\n            preds = preds.cpu().numpy() + 1  # Adjust back to original label indexing if necessary\n\n            for id_, pred in zip(ids, preds):\n                predictions[id_] = pred\n\n    return predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = predict(model, test_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_preds(df_train, df_test, text_column, source:str, avg='mean'):\n    y_train = df_train['AdoptionSpeed']\n    X_train = df_train[text_column]\n    \n    X_test = df_test['Description']\n    \n    X_train = X_train.apply(word_tokenize).apply(lambda x: tok2vec(x, source, avg)).to_numpy()\n    X_test = X_test.apply(word_tokenize).apply(lambda x: tok2vec(x, source, avg)).to_numpy()\n    \n    X_train = np.stack(X_train, axis=0)\n    X_test = np.stack(X_test, axis=0)\n    \n    model = LogisticRegression(max_iter=1000, random_state=42)\n    model.fit(X_train, y_train)\n\n    predictions_prob = model.predict_proba(X_test)\n    predictions = np.argmax(predictions_prob, axis=1) + 1\n\n    return predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predictions_final = get_preds(df, df_test, 'Description', source='word2vec')\n\n# predictions_final","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Count the frequency of each value for both datasets\n# counter_1 = Counter(predictions_final)\n# counter_2 = Counter(predictions_final)\n\n# # Sort the data based on the x-axis (values)\n# sorted_data_1 = sorted(counter_1.items())  # List of tuples (value, count)\n# sorted_data_2 = sorted(counter_2.items())  # List of tuples (value, count)\n\n# # Unpack the sorted values and counts\n# x_values_1, y_counts_1 = zip(*sorted_data_1)\n# x_values_2, y_counts_2 = zip(*sorted_data_2)\n\n# # Plotting the first dataset\n# plt.bar(x_values_1, y_counts_1, width=0.4, color='blue', align='center', label='Original')\n\n# # Plotting the second dataset with a slight shift on the x-axis for better visibility\n# plt.bar([x for x in x_values_2], y_counts_2, width=0.4, color='orange', align='center', label='Without Outliers')\n\n# # Adding labels, legend, and title\n# plt.xlabel('Values')\n# plt.ylabel('Count of Instances')\n# plt.title('Distribution of AdoptionSpeed')\n# plt.legend()\n\n# # Display the plot\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(predictions_final)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the final submission file","metadata":{}},{"cell_type":"code","source":"predictions_final = test_predictions","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['AdoptionSpeed'] = predictions_final","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.hist('AdoptionSpeed')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_submission = df_test.drop('Description', axis=1)\n\nprint(final_submission)\nfinal_submission.hist('AdoptionSpeed')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_submission.to_csv(os.path.join(CFG.OUTPUT_DIR, 'submission.csv'), index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}