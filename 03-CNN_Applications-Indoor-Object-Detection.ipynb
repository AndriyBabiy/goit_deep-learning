{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3970170,"sourceType":"datasetVersion","datasetId":2356282}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/andriybabiy/03-cnn-applications-indoor-object-detection?scriptVersionId=201094349\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 03-CNN_Applications-Indoor-Object-Detection\nUsing the indoor-object-detection dataset, my goal is to develop and train a deep learining model that will recognise indoor items from an image.","metadata":{}},{"cell_type":"markdown","source":"# Importing the required libraries","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade ultralytics\n!pip install --upgrade -U ray[tune]\n!pip install -U ipywidgets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-09T19:56:07.853619Z","iopub.execute_input":"2024-10-09T19:56:07.854044Z","iopub.status.idle":"2024-10-09T19:56:44.395378Z","shell.execute_reply.started":"2024-10-09T19:56:07.853997Z","shell.execute_reply":"2024-10-09T19:56:44.394247Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport re\nimport glob\nimport random\nimport yaml\n\nimport torch\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\nfrom PIL import Image\nimport cv2\n\nfrom ultralytics import YOLO\n\n%matplotlib inline\n\n! wandb disabled","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:44.397911Z","iopub.execute_input":"2024-10-09T19:56:44.398654Z","iopub.status.idle":"2024-10-09T19:56:49.600786Z","shell.execute_reply.started":"2024-10-09T19:56:44.398606Z","shell.execute_reply":"2024-10-09T19:56:49.599735Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing and preparing the dataset of images","metadata":{}},{"cell_type":"code","source":"class CFG:\n    DEBUG = False\n    FRACTION = 0.05 if DEBUG else 1.0\n    SEED = 42\n    \n    CLASSES = ['door',\n               'cabinetDoor',\n               'refrigeratorDoor',\n               'window',\n               'chair',\n               'table',\n               'cabinet',\n               'couch',\n               'openedDoor',\n               'pole']\n    NUM_CLASSES_TO_TRAIN = len(CLASSES)\n    \n    EPOCHS = 3 if DEBUG else 100 #70\n    BATCH_SIZE = 16\n    \n    BASE_MODEL = 'yolov9s'\n    BASE_MODEL_WEIGHTS = f'{BASE_MODEL}.pt'\n    EXP_NAME = f'ppe_css_{EPOCHS}_epochs'\n    \n    OPTIMIZER = 'auto'\n    LR = 1e-5\n    LR_FACTOR = 0.01\n    WEIGHT_DECAY = 5e-4\n    DROPOUT = 0.0\n    PATIENCE = 30\n    PROFILE = False\n    LABEL_SMOOTHING = 0.0\n    \n    CUSTOM_DATASET_DIR = '/kaggle/input/indoor-object-detection'\n    OUTPUT_DIR = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:49.60241Z","iopub.execute_input":"2024-10-09T19:56:49.603116Z","iopub.status.idle":"2024-10-09T19:56:49.610587Z","shell.execute_reply.started":"2024-10-09T19:56:49.603081Z","shell.execute_reply":"2024-10-09T19:56:49.609715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dict_file = {\n    'train': os.path.join(CFG.CUSTOM_DATASET_DIR, 'train'),\n    'val': os.path.join(CFG.CUSTOM_DATASET_DIR, 'valid'),\n    'test': os.path.join(CFG.CUSTOM_DATASET_DIR, 'test'),\n    'nc': CFG.NUM_CLASSES_TO_TRAIN,\n    'names': CFG.CLASSES\n    }\n\nwith open(os.path.join(CFG.OUTPUT_DIR, 'data.yaml'), 'w+') as file:\n    yaml.dump(dict_file, file)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:49.612025Z","iopub.execute_input":"2024-10-09T19:56:49.612891Z","iopub.status.idle":"2024-10-09T19:56:49.625995Z","shell.execute_reply.started":"2024-10-09T19:56:49.612845Z","shell.execute_reply":"2024-10-09T19:56:49.625223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### read yaml file created\ndef read_yaml_file(file_path = CFG.CUSTOM_DATASET_DIR):\n    with open(file_path, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n            return data\n        except yaml.YAMLError as e:\n            print(\"Error reading YAML:\", e)\n            return None\n\n### print it with newlines\ndef print_yaml_data(data):\n    formatted_yaml = yaml.dump(data, default_style=False)\n    print(formatted_yaml)\n\nfile_path = os.path.join(CFG.OUTPUT_DIR, 'data.yaml')\nyaml_data = read_yaml_file(file_path)\n\nif yaml_data:\n    print_yaml_data(yaml_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:49.62867Z","iopub.execute_input":"2024-10-09T19:56:49.62896Z","iopub.status.idle":"2024-10-09T19:56:49.640224Z","shell.execute_reply.started":"2024-10-09T19:56:49.628928Z","shell.execute_reply":"2024-10-09T19:56:49.639348Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Looking into the dataset components","metadata":{}},{"cell_type":"code","source":"def display_image(image, print_info = True, hide_axis = False):\n    if isinstance(image, str):  # Check if it's a file path\n        img = Image.open(image)\n        plt.imshow(img)\n    elif isinstance(image, np.ndarray):  # Check if it's a NumPy array\n        image = image[..., ::-1]  # BGR to RGB\n        img = Image.fromarray(image)\n        plt.imshow(img)\n    else:\n        raise ValueError(\"Unsupported image format\")\n\n    if print_info:\n        print('Type: ', type(img), '\\n')\n        print('Shape: ', np.array(img).shape, '\\n')\n\n    if hide_axis:\n        plt.axis('off')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:49.641399Z","iopub.execute_input":"2024-10-09T19:56:49.641698Z","iopub.status.idle":"2024-10-09T19:56:49.652798Z","shell.execute_reply.started":"2024-10-09T19:56:49.641663Z","shell.execute_reply":"2024-10-09T19:56:49.651971Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dict_file['train']","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:49.653881Z","iopub.execute_input":"2024-10-09T19:56:49.654844Z","iopub.status.idle":"2024-10-09T19:56:49.665559Z","shell.execute_reply.started":"2024-10-09T19:56:49.654809Z","shell.execute_reply":"2024-10-09T19:56:49.664786Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_image_path = f'{CFG.CUSTOM_DATASET_DIR}/train/images/000bf0ddff4c7310.jpg'\n\ndisplay_image(example_image_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:49.666825Z","iopub.execute_input":"2024-10-09T19:56:49.667182Z","iopub.status.idle":"2024-10-09T19:56:50.169842Z","shell.execute_reply.started":"2024-10-09T19:56:49.667141Z","shell.execute_reply":"2024-10-09T19:56:50.168805Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_random_images_from_folder(folder_path, num_images=20, seed=CFG.SEED):\n    random.seed(seed)\n    \n    # Get a list of image files in the folder\n    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png', '.jpeg', '.gif'))]\n    \n    # Ensure that we have at least num_images files to choose from \n    if len(image_files) < num_images: \n        raise ValueError(\"Not enough images in the folder\")\n    \n    # Randomly select num_images image files\n    selected_files = random.sample(image_files, num_images)\n    \n    # Create a subplot grid\n    num_cols = 5\n    num_rows = (num_images + num_cols - 1) // num_cols\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n    \n    for i, file_name in enumerate(selected_files):\n        # Open and display the images using PIL\n        img = Image.open(os.path.join(folder_path, file_name))\n        \n        if num_rows == 1:\n            ax = axes[i % num_cols]\n        else:\n            ax = axes[i // num_cols, i % num_cols]\n            \n        ax.imshow(img)\n        ax.axis('off')\n        \n    # Remove empty subplots\n    for i in range(num_images, num_rows * num_cols):\n        if num_rows == 1:\n            fig.delaxes(axes[i % num_cols])\n        else:\n            fig.delaxes(axes[i // num_cols, i % num_cols])\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:50.170933Z","iopub.execute_input":"2024-10-09T19:56:50.171219Z","iopub.status.idle":"2024-10-09T19:56:50.180952Z","shell.execute_reply.started":"2024-10-09T19:56:50.171181Z","shell.execute_reply":"2024-10-09T19:56:50.180007Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folder_path = CFG.CUSTOM_DATASET_DIR + '/train/images'\nplot_random_images_from_folder(folder_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:50.182169Z","iopub.execute_input":"2024-10-09T19:56:50.18258Z","iopub.status.idle":"2024-10-09T19:56:53.119326Z","shell.execute_reply.started":"2024-10-09T19:56:50.182547Z","shell.execute_reply":"2024-10-09T19:56:53.118193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_image_properties(image_path):\n    # Read the image file\n    img = cv2.imread(image_path)\n    \n    # Check if the image file is read correctly\n    if img is None:\n        raise ValueError(\"Could not read image file\")\n        \n    properties = {\n        \"width\": img.shape[1],\n        \"height\": img.shape[0],\n        \"channels\": img.shape[2] if len(img.shape) == 3 else 1,\n        \"dtype\": img.dtype\n    }\n    \n    return properties","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:53.120502Z","iopub.execute_input":"2024-10-09T19:56:53.120824Z","iopub.status.idle":"2024-10-09T19:56:53.126964Z","shell.execute_reply.started":"2024-10-09T19:56:53.12079Z","shell.execute_reply":"2024-10-09T19:56:53.126002Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_properties = get_image_properties(example_image_path)\nimg_properties","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:53.128164Z","iopub.execute_input":"2024-10-09T19:56:53.128566Z","iopub.status.idle":"2024-10-09T19:56:53.14519Z","shell.execute_reply.started":"2024-10-09T19:56:53.128532Z","shell.execute_reply":"2024-10-09T19:56:53.144259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_idx = {str(i): CFG.CLASSES[i] for i in range(CFG.NUM_CLASSES_TO_TRAIN)}\n\nclass_stat = {}\ndata_len = {}\nclass_info = []\n\n    \nfor mode in ['train', 'valid', 'test']:\n    class_count = {CFG.CLASSES[i]: 0 for i in range(CFG.NUM_CLASSES_TO_TRAIN)}\n\n    path = os.path.join(CFG.CUSTOM_DATASET_DIR, mode, 'labels')\n\n    for file in os.listdir(path):\n        with open(os.path.join(path, file)) as f:\n            lines = f.readlines()\n\n            for cls in set([line[0] for line in lines]):\n                class_count[class_idx[cls]] += 1\n\n    data_len[mode] = len(os.listdir(path))\n    class_stat[mode] = class_count\n\n    class_info.append({'Mode': mode, **class_count, 'Data_Volume': data_len[mode]})\n            \ndataset_stats_df = pd.DataFrame(class_info)\nwith pd.option_context('display.max_columns', None):\n    display(dataset_stats_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:53.146467Z","iopub.execute_input":"2024-10-09T19:56:53.147032Z","iopub.status.idle":"2024-10-09T19:56:53.746873Z","shell.execute_reply.started":"2024-10-09T19:56:53.146991Z","shell.execute_reply":"2024-10-09T19:56:53.745961Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, mode in enumerate(['train', 'valid', 'test']):\n    sns.barplot(\n        data=dataset_stats_df[dataset_stats_df['Mode'] == mode].drop(columns=\"Mode\"),\n        orient='v',\n        ax=axes[i],\n        palette='Set2'        \n    )\n    \n    axes[i].set_title(f'{mode.capitalize()} Class Statistics')\n    axes[i].set_xlabel('Classes')\n    axes[i].set_ylabel('Count')\n    axes[i].tick_params(axis='x', rotation=90)\n    \n    # add annotations on top of each bar\n    for p in axes[i].patches:\n        axes[i].annotate(f\"{int(p.get_height())}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n                         ha='center', va='center', fontsize=8, color='black', xytext=(0, 5),\n                         textcoords='offset points')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:53.750955Z","iopub.execute_input":"2024-10-09T19:56:53.751257Z","iopub.status.idle":"2024-10-09T19:56:54.82351Z","shell.execute_reply.started":"2024-10-09T19:56:53.751224Z","shell.execute_reply":"2024-10-09T19:56:54.822472Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training the model.","metadata":{}},{"cell_type":"code","source":"model = YOLO(CFG.BASE_MODEL_WEIGHTS)\n\nresults = model.predict(\n    source = example_image_path,\n    \n    classes = [0],\n    conf = 0.30,\n#     device = [0, 1], # inference with dual GPU\n    device = None, # inference with GPU\n    imgsz = (img_properties['height'], img_properties['width']),\n    \n    save = True,\n    save_txt = True,\n    save_conf = True,\n    exist_ok = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:54.824649Z","iopub.execute_input":"2024-10-09T19:56:54.824948Z","iopub.status.idle":"2024-10-09T19:56:58.632289Z","shell.execute_reply.started":"2024-10-09T19:56:54.824916Z","shell.execute_reply":"2024-10-09T19:56:58.631383Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_image_inference_output = example_image_path.split('/')[-1]\ndisplay_image(f'runs/detect/predict/{example_image_inference_output}')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:58.633576Z","iopub.execute_input":"2024-10-09T19:56:58.634026Z","iopub.status.idle":"2024-10-09T19:56:59.084738Z","shell.execute_reply.started":"2024-10-09T19:56:58.633989Z","shell.execute_reply":"2024-10-09T19:56:59.083644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Model:', CFG.BASE_MODEL_WEIGHTS)\nprint('Epochs: ', CFG.EPOCHS)\nprint('Batch: ', CFG.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:59.086054Z","iopub.execute_input":"2024-10-09T19:56:59.086412Z","iopub.status.idle":"2024-10-09T19:56:59.092666Z","shell.execute_reply.started":"2024-10-09T19:56:59.086374Z","shell.execute_reply":"2024-10-09T19:56:59.091642Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO(CFG.BASE_MODEL_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:59.094144Z","iopub.execute_input":"2024-10-09T19:56:59.094567Z","iopub.status.idle":"2024-10-09T19:56:59.445646Z","shell.execute_reply.started":"2024-10-09T19:56:59.094513Z","shell.execute_reply":"2024-10-09T19:56:59.444547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def cleanup_memory_callback(trainer):\n#     gc.collect()\n#     torch.cuda.empty_cache()\n#     print(\"Memory cleaned after epoch\")\n\ndef cleanup_before_training():\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"Memory cleaned befort training\")","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:59.447084Z","iopub.execute_input":"2024-10-09T19:56:59.447514Z","iopub.status.idle":"2024-10-09T19:56:59.452971Z","shell.execute_reply.started":"2024-10-09T19:56:59.447467Z","shell.execute_reply":"2024-10-09T19:56:59.451816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ncleanup_before_training()\nos.environ['WANDB_MODE'] = 'offline'\n\n### train\nmodel.train(\n    data = os.path.join(CFG.OUTPUT_DIR, 'data.yaml'),\n\n    task = 'detect',\n\n    imgsz = (img_properties['height'], img_properties['width']),\n\n    epochs = CFG.EPOCHS,\n    batch = CFG.BATCH_SIZE,\n    optimizer = CFG.OPTIMIZER,\n    lr0 = CFG.LR,\n    lrf = CFG.LR_FACTOR,\n    weight_decay = CFG.WEIGHT_DECAY,\n    dropout = CFG.DROPOUT,\n    fraction = CFG.FRACTION,\n    patience = CFG.PATIENCE,\n    profile = CFG.PROFILE,\n    label_smoothing = CFG.LABEL_SMOOTHING,\n\n    name = f'{CFG.BASE_MODEL}_{CFG.EXP_NAME}',\n    seed = CFG.SEED,\n    \n    val = True,\n    amp = True,    \n    exist_ok = True,\n    resume = False,\n    device = [0], \n#     device = None, # CPU run\n    verbose = False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T19:56:59.454482Z","iopub.execute_input":"2024-10-09T19:56:59.454882Z","iopub.status.idle":"2024-10-09T22:09:50.522663Z","shell.execute_reply.started":"2024-10-09T19:56:59.454818Z","shell.execute_reply":"2024-10-09T22:09:50.518671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.export(\n    format = 'onnx',\n    imgsz = (img_properties['height'], img_properties['width']),\n    half = False,\n    int8 = False,\n    simplify = False,\n    nms = False\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:09:50.524558Z","iopub.execute_input":"2024-10-09T22:09:50.525111Z","iopub.status.idle":"2024-10-09T22:09:57.094466Z","shell.execute_reply.started":"2024-10-09T22:09:50.525052Z","shell.execute_reply":"2024-10-09T22:09:57.093523Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results_paths = [\n    i for i in \n    glob.glob(f'{CFG.OUTPUT_DIR}runs/detect/{CFG.BASE_MODEL}_{CFG.EXP_NAME}/*.png') +\n    glob.glob(f'{CFG.OUTPUT_DIR}runs/detect/{CFG.BASE_MODEL}_{CFG.EXP_NAME}/*.png')\n    if 'batch' not in i\n]\n\nresults_paths","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:09:57.09555Z","iopub.execute_input":"2024-10-09T22:09:57.095834Z","iopub.status.idle":"2024-10-09T22:09:57.104216Z","shell.execute_reply.started":"2024-10-09T22:09:57.095804Z","shell.execute_reply":"2024-10-09T22:09:57.10329Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for file in sorted(results_paths):\n    print(file)\n    display_image(file, print_info = False, hide_axis = True)\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:09:57.105467Z","iopub.execute_input":"2024-10-09T22:09:57.106258Z","iopub.status.idle":"2024-10-09T22:10:05.451348Z","shell.execute_reply.started":"2024-10-09T22:09:57.106212Z","shell.execute_reply":"2024-10-09T22:10:05.450409Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(f'{CFG.OUTPUT_DIR}runs/detect/{CFG.BASE_MODEL}_{CFG.EXP_NAME}/results.csv')\ndf = df.rename(columns=lambda x: x.replace(\" \", \"\"))\ndf.to_csv(f'{CFG.OUTPUT_DIR}training_log_df.csv', index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:10:05.452563Z","iopub.execute_input":"2024-10-09T22:10:05.452884Z","iopub.status.idle":"2024-10-09T22:10:05.493546Z","shell.execute_reply.started":"2024-10-09T22:10:05.45285Z","shell.execute_reply":"2024-10-09T22:10:05.492607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('*'*50)\nprint('\\nBest Training Box loss: ', df['train/box_loss'].min(), ', on epoch: ', df['train/box_loss'].argmin() + 1, '\\n')\nprint('\\nBest Validation Box loss: ', df['val/box_loss'].min(), ', on epoch: ', df['val/box_loss'].argmin() + 1, '\\n')\n\nprint('='*50)\nprint('\\nBest Training Cls loss: ', df['train/cls_loss'].min(), ', on epoch: ', df['train/cls_loss'].argmin() + 1, '\\n')\nprint('\\nBest Validation Cls loss: ', df['val/cls_loss'].min(), ', on epoch: ', df['val/cls_loss'].argmin() + 1, '\\n')\n\nprint('='*50)\nprint('\\nBest Training DFL loss: ', df['train/dfl_loss'].min(), ', on epoch: ', df['train/dfl_loss'].argmin() + 1, '\\n')\nprint('\\nBest Validation DFL loss: ', df['val/dfl_loss'].min(), ', on epoch: ', df['val/dfl_loss'].argmin() + 1, '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:10:05.494953Z","iopub.execute_input":"2024-10-09T22:10:05.495664Z","iopub.status.idle":"2024-10-09T22:10:05.505588Z","shell.execute_reply.started":"2024-10-09T22:10:05.495613Z","shell.execute_reply":"2024-10-09T22:10:05.504673Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n\n### Training and Validation box_loss\nax1.set_title('Box Loss')\nax1.plot(df['epoch'], df['train/box_loss'], label='Training box_loss', marker='o', linestyle='-')\nax1.plot(df['epoch'], df['val/box_loss'], label='Validation box_loss', marker='o', linestyle='-')\nax1.set_ylabel('Box Loss')\nax1.legend()\nax1.grid(True)\n\n### Training and Validation cls_loss\nax2.set_title('Cls Loss')\nax2.plot(df['epoch'], df['train/cls_loss'], label='Training cls_loss', marker='o', linestyle='-')\nax2.plot(df['epoch'], df['val/cls_loss'], label='Validation cls_loss', marker='o', linestyle='-')\nax2.set_ylabel('cls_loss')\nax2.legend()\nax2.grid(True)\n\n### Training and Validation dfl_loss\nax3.set_title('DFL Loss')\nax3.plot(df['epoch'], df['train/dfl_loss'], label='Training dfl_loss', marker='o', linestyle='-')\nax3.plot(df['epoch'], df['val/dfl_loss'], label='Validation dfl_loss', marker='o', linestyle='-')\nax3.set_xlabel('Epochs')\nax3.set_ylabel('dfl_loss')\nax3.legend()\nax3.grid(True)\n\nplt.suptitle('Training Metrics vs. Epochs')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:10:05.506725Z","iopub.execute_input":"2024-10-09T22:10:05.507148Z","iopub.status.idle":"2024-10-09T22:10:06.219086Z","shell.execute_reply.started":"2024-10-09T22:10:05.507087Z","shell.execute_reply":"2024-10-09T22:10:06.218141Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_results_paths = [\n    i for i in\n    glob.glob(f'{CFG.OUTPUT_DIR}runs/detect/{CFG.BASE_MODEL}_{CFG.EXP_NAME}/*.png') +\n    glob.glob(f'{CFG.OUTPUT_DIR}runs/detect/{CFG.BASE_MODEL}_{CFG.EXP_NAME}/*.jpg')\n    if 'val_batch' in i\n]\n\nlen(validation_results_paths)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:10:06.220605Z","iopub.execute_input":"2024-10-09T22:10:06.221358Z","iopub.status.idle":"2024-10-09T22:10:06.229728Z","shell.execute_reply.started":"2024-10-09T22:10:06.221292Z","shell.execute_reply":"2024-10-09T22:10:06.228947Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if len(validation_results_paths) >= 1:\n    print(validation_results_paths[-1])","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:10:06.230917Z","iopub.execute_input":"2024-10-09T22:10:06.231205Z","iopub.status.idle":"2024-10-09T22:10:06.238499Z","shell.execute_reply.started":"2024-10-09T22:10:06.231174Z","shell.execute_reply":"2024-10-09T22:10:06.237581Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### check predictions or labels from a random validation batch\nif len(validation_results_paths) >= 1:\n    val_img_path = random.choice(validation_results_paths)\n    print(val_img_path)\n    display_image(val_img_path, print_info = False, hide_axis = True)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T22:10:06.239617Z","iopub.execute_input":"2024-10-09T22:10:06.239926Z","iopub.status.idle":"2024-10-09T22:10:06.735556Z","shell.execute_reply.started":"2024-10-09T22:10:06.239893Z","shell.execute_reply":"2024-10-09T22:10:06.734587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Result analysis","metadata":{}},{"cell_type":"markdown","source":"In this analysis, using the indoor-object-detection dataset and the YOLO model it is evident that the convolutional neural network for classification of elments found in the images.\n\nFollowing the initial running of the model the results were that the model worked but the accuracy was quite low, here I trained the model using 100 epochs with the YOLOv8s model the ultimate result showed an F1 confidence of 42% at a confidence level of around 20%. This was following around 40minues of training required to train the model. This earlier version of YOLO also showed that there were many of the smaller elements that were often confused for the background. However the learning curves for the \n\nFollowing this result I devided to once agin run the analysis using a newer version being YOLOv9s. This trianing was similarly training for 100 epochs, however took 2.2hours. This newer model showed slightly improved results in the space of the F1-Confidence curves with the model being able to accurately identify 45% of the items at a 23% confidence, this is an improvement on the previous model however shows that there is still a lot of room for imporvement. For the F1, precision and recall graphs it is evident that there are classes that are outperforming (cabinetDoor and refrigeratorDoor) who are getting F1 scores close to 80% showing that the classes with more instances have better overall perfromance. This behaviour is also evident in the normalized confusion matrix graphs as the classes that have low represntation in the dataset are often confused with the background. Another interesting ocurrence is that the openDoor and door class are often confused with each other in the normalized confusion matrix showing that there is possibly training ocurring based on the general shape of the door including the frame which causes the confusion.\n\nThe loss function grpahs also show that there is some overtraining ocurring as the box loss and cls loss are flattening on the test data while they are decreasing in the train data, the overfitting is also most evident in the DFL loss, which is actually becoming worse over time showing that the extra training on the train data is not generalizing to the test dataset.\n\nBased on these elements, the conculsion is that the training was adequate for some of the classes but not for all of them. In order to further improve the performance of the model there is room to decrease the learning rate from the current 1e-5 and to increase the amount of epochs. This will increase the training time, however may bring the results needed. This recommendation is based on the volatile graphs seen in the percision and recall metrics graphs. Another possible area would be to increase the amount of images in the training dataset using image transformation as due to the low amount of some classes, there were issues with the training.\n","metadata":{}}]}